Agent Semantics, Semantic Spacetime, and
Graphical Reasoning
Mark Burgess

arXiv:2506.07756v2 [cs.AI] 13 Jun 2025

June 16, 2025
Abstract
Some formal aspects of the Semantic Spacetime graph model are presented, with reference to its
use for directed knowledge representations and capacity for process modelling. A finite γ(3, 4) representation is defined to form a closed scaffolding set of operations that can scale to any degree of
semantic complexity, with a highly simple rule set and a universal ontology. The Semantic Spacetime postulates bring predictability when reasoning, with minimal constraints on following pathways
in graphs. The ubiquitous appearance of absorbing states in any partial graph means that certain graph
processes leak information and represent entropy changing processes. The issue is closely associated
with the issue of division by zero, which signals a loss of closure and the need for manual injection of
remedial information. The Semantic Spacetime model (and its Promise Theory) origins help to clarify
how such absorbing states are associated with boundary information where intentionality can enter.

Contents
1

Introduction
1.1 Spacetime characteristics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1.2 Euclidean vs graph . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1.3 Symmetries . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

2

Semantics of spacetime and spacetime semantics
2.1 The purpose of Semantic Spacetime . . . . . . . . . . . . . . . . . . . .
2.2 The 4 relationship types . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.3 Resolving link type ambiguities with 3 types: events, things, and concepts
2.4 The combined Γ(N, L) 7→ γ(3, 4) representation . . . . . . . . . . . . .
2.5 Strategy in semantic graph representation . . . . . . . . . . . . . . . . .
2.6 Directional inference in the γ(3, 4) representation . . . . . . . . . . . . .

3

2
3
4
5

.
.
.
.
.
.

.
.
.
.
.
.

5
6
6
8
8
10
14

Graph-algebraic Semantic Spacetime
3.1 Proper names as semantic coordinates . . . . . . . . . . . . . . . . . . . . . . . . . .
3.2 Graph structures: appointments and loops . . . . . . . . . . . . . . . . . . . . . . . .
3.3 Propagation and terminating absorption of the γ(3, 4) types . . . . . . . . . . . . . . .
3.4 Allowed type transitions of γ(3, 4) . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3.5 Inference rules and symmetries . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3.6 Absorption by blind alley and by statistical aggregation: scales and degrees of freedom

.
.
.
.
.
.

15
15
16
17
18
19
20

1

.
.
.
.
.
.

.
.
.
.
.
.

.
.
.
.
.
.

.
.
.
.
.
.

.
.
.
.
.
.

.
.
.
.
.
.

4

5

6

Matrix representations of graphs
4.1 Matrices for γ(3, 4) skeleton . . . . . . . . . . . . . . . . . . .
4.2 Stable regions of a graph, information, emission and absorption .
4.3 Sources and sinks . . . . . . . . . . . . . . . . . . . . . . . . .
4.4 Frobenious-Perron eigenvector theorem . . . . . . . . . . . . .

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

22
23
24
25
27

Graph semantics of operational arithmetic computation
5.1 Arithemetic . . . . . . . . . . . . . . . . . . . . . .
5.2 Special arithmetic states 1 and 0 . . . . . . . . . . .
5.3 Addition as a graphical process . . . . . . . . . . . .
5.4 Subtraction as a graphical process . . . . . . . . . .
5.5 Multiplication as a graphical process . . . . . . . . .
5.6 Division . . . . . . . . . . . . . . . . . . . . . . . .
5.7 Division by zero semantics . . . . . . . . . . . . . .

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

28
30
30
31
33
34
34
36

Conclusions

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

38

A Examples

1

.
.
.
.
.
.
.

42

Introduction

Semantic Spacetime (SST) is a discrete, graph theoretic ‘agent’ representation of configurations and
process phenomena, used for modelling scenarios that include knowledge representations, in the form of
labelled directed graphs [1–4]. It enables both qualitative and quantitative interpretations of processes by
combining physical and virtual concepts (from physics and information science) into a Promise Theoretic
agent model [5]. Promise Theory principles emphasize the autonomy or locality of causal behaviour, so
there are clear motivations for modelling phenomena in this way. As a graph theoretical structure, a
Semantic Spacetime is a collection of nodes (agents) joined by links (channels for process information),
both of which may have annotations and numerical values associated with them.
A key application for Semantic Spacetime in artificial systems is to represent ‘knowledge’ (in its
simplified sense) and process structures, such as those normally associated with indexing methods or Semantic Webs, like the triple store approaches of the Resource Description Framework (RDF) [6]. Further,
one can explore how conceptual meanings and themes arise from these ground states [7, 8]. These applications are the subject of many papers spanning many decades, from the beginnings of taxonomies in the
biosciences to database schemas to tuplet spaces with supporting ontologies. The persistent difficulties
and logical inconsistencies of these early methods, along with their lack of scalability, can be traced back
to the adoption of an entity-centric modelling of the world in terms of ‘nouns’ and ‘verbs’ on a single
scale—an affectation that probably stems from modern philosophy. The use of ontologies, in particular,
to impose constraints on meaning at the level of logic is fraught with rigidity and consistency issues,
because ontologies do not employ principles rooted in the processes of the world.
Processes are sequences of steps, akin to algorithmic data transformations, consisting of states unfolding over time as a set of ‘states’ in the most general sense. Processes embody everything from
manufacturing (operations on raw materials) to simple arithmetic (operations on numbers). Our goal in
representing knowledge is to model the changes to every volume of spacetime we care about in terms of
its role in a cohesive semantic process that we can use as a model for reasoning. Semantic Spacetime
aims to be sufficiently formal to be precise and yet sufficiently flexible to be widely useful.

2

1.1

Spacetime characteristics

A spacetime has certain regular concepts and ‘functional’ behaviours such as distance, time, scale, containment, direction: forwards, backwards and so on. In addition, other physical concepts such as symmetry and order can be applied to spacetime ‘states’ and the operations involving them.
The concept of ‘functional semantics’ [9] is not a topic normally associated with natural sciences like
physics, except perhaps for a small number of category theorists [10, 11]. In linguistics, on the other
hand, semantics are closely allied with semiotics of meaning [12, 13]. In Computer Science, semantics
have a more pragmatic meaning: related to the outcomes of algorithms, i.e. what behaviour or response
is implied by certain concepts and types. In addition to this, a meaning closer to the linguistic one is used
in connection with knowledge representations and ontology [14].
Although the terminology of ‘semantics’ may be somewhat frowned upon and do not apparently play
a prominent role in discussions of natural science, they are very much in evidence just below the surface
in the form of roles and representations. In the traditional mathematics of rings, fields, and groups,
for example, sometimes x is a quantity of something, with mundane semantics (what one would call a
number), but in other cases x can be part of a generating function—an obstable used as part of, say, a
differential operator scheme for algorithmically for representing certain patterns and process fragments,
e.g. as in the partition functions of thermodynamics and quantum field theory, or the ladder operations
of groups and graphs. Graphs therefore play a natural bridging role between disciplines and we should
include some discussion of how visual imagery is used to give meaning to even abstract concepts and
procedures.
Spacetime can certainly be described in terms of concepts then, but here we are equally interested
in the converse. If we turn this association around, we can explore the way those concepts are perhaps
derived from spacetime concepts. After all, earlier in our evolutionary journey there was nothing else
to describe except where things were, how to find them, and basic process impetus like survival. There
are likely insights that can be applied to other areas like knowledge representation and reasoning. The
chief reason for wanting to do this is the idea that human concepts ultimately derive from concepts about
space and time. Consider the following phrases, each steeped in spacetime metaphors and yet apparently
representing quite different meanings:
Christmas is fast approaching.
We are getting close to an answer.We are approaching a result.
A long time coming.
The conclusion is just around the corner.
It’s a far far better thing.
A sufficiently large contribution.
Time flies like an arrow.
The moment of truth approaches.
The subject is heavy reading.
Just north of 200 dollars.
Within the bounds of reason.
What makes it tick?
I would point out.
At this point of the proceedings.
It then took a left turn.
That’s close enough to a win.
A long running affair.
On some level.
Just in time, within acceptable limits.
How long will you be?
3

We are moving ahead in spite of setbacks.
Our position is clear.
These are examples of how we use spacetime semantics at the very root of meaning. In addition, the
gradual erosion of literal meanings and normalization of metaphors that are later forgotten is a plausible
model of how language evolves in the first place [7, 8, 15]. All this is a good reason to explore the idea
that descriptions of events and the knowledge associated with them can be simplified by appealing to the
underlying spacetime types and how they are used to express meaning. It requires a more expansive and
open minded approach to meaning than is common in either physics or linguistics, yet this is what we
attempt to give a more formal substantial meaning to here.

1.2

Euclidean vs graph

The Euclidean-Cartesian view of space is the most prevalent in science. Space is associated with translations in directions x, y, z etc. The Minkowski space generalizes this to include coordinates for an
observer’s local time too. This view stems from the ballistic origins of physics as a way of predicting the
trajectories of weapons fire. Occasionally, one might use radial coordinates from a localized point when
considering perimeter boundaries, orbits, and Gaussian enclosures and so forth. However, the concept of
translational invariance of space is engineered so deeply into our tools that any deviation from symmetry
group thinking feels like a major deviation from common sense. In the Newtonian tradition, we separate
what happens in space from the theatrical enclosure of abstract space itself.
The classical view of spacetime attributes all phenomena to the exterior space of points. Nothing happens ‘within’ points. Only in the quantum theory do we encounter interior states, such as wave functions,
field values, spin, and other quantum numbers. The idea of hidden dimensions amounts to interior degrees
of freedom at each point. Together with agent models, quantum theory sees each location in spacetime as
having intrinsic or ‘interior’ properties that may result from unobservable processes within, but may also
engage in more classical cooperative processes between agents in an exterior ballistic view.
A graph Γ = (N, L) is a pair of sets N of nodes (also called vertices) representing locations and
L of links (also called edges). Graph oriented spacetime has rather different challenges as compared to
the more familiar continuum spacetime we are taught in school. In a continuum, everything is about the
continuity. The concept of infinity lies embedded in nearly every assumption, without a clear sense of
how. In a continuum, there is no reason for one location (node) to point explicitly to another as in a graph.
One doesn’t need a reason for the next point to be there–indeed, one rather needs a reason why it might
not be connected and ready to receive some material body. There are many aspects of Newtonian age
physics that one takes for granted that fail to make certain sense on closer examination. The continuum
limit enables these slights of hand in a way which is both elegant and audacious [16].
Arrows may be used arbitrarily in as labelling devices, but in the world of causal events there are
different kinds of graph. Petri Nets, bigraphs, and Labelled Transition Systems are examples of attempts
to model process order as graphs, for instance. On the other hand, the standard Entity-Relation model
of SQL databases uses arrows to represent attributes, and uses counters to represent order. In Semantic
Spacetime one makes no rules about this; instead, some simple principles keep the meanings of arrows
and attributes clear with a weak implicit typing.
The relationship between graphs and continuum spaces in the literature may lead to confusion. Embedding of a graph within a Euclidean space is how one often gives meaning to concepts like distance and
dimension in graphs. The lattice approximations to Euclidean space uses regular graphs with long range
order to emulate Euclidean space with graphs. Triangulation graphs can form approximate coverings of
general manifolds. These ideas are not how we are to understand Semantic Spacetime: we are not trying
to construct a skeletal embedding of trajectories in a Euclidean theatre.
Similarly, we are not looking to yield a classification tree such as a taxonomy, which is a hierarchical
tree for organizing membership of sets, subsets, collections, or grouping as a form of containment. There,
4

parent nodes branch out, forming a semantic branching process [17]. The fact that all instances express
a conceptual similarity gives meaning to parent-child relationships. A taxonomy is an interior spanning
tree for internal concepts. Containment of things is a way of using a hierarchy because partitioning of
space follows a fractal branching rule. Graphs therefore unify loosely topological and strictly geometrical
concepts. Semantic spacetime has aspects of both conceptual decomposition and spanning of geometry.
It is not a taxonomy or an ontology, though we may find those embedded within it. It is neither a roadmap
embedded within Euclidean space with many redundant points in between. One has only nodes and links
with interior values.
Embeddings interior and exterior to nodes are used widely in knowledge engineering. This is how
feature vectors are introduced as interior spaces in artificial neural network models (see figure 6) [18]. In
the world of ANNs, there are two underlying spaces: the network of the ANN and the feature vector space
that is trained to represent concepts. This is almost the opposite of the model of physics where Euclidean
space is the embedding for processes. There are many attempts to formulate a mathematical description
of the dynamics of these representations, but they are inscrutable property models [19]. In SST we take
a more symbolic algebraic approach to semantics, closer to language encoding than a statistical model of
meaning.

1.3

Symmetries

The role of symmetry is spacetime is to seek a minimalism of description by emphasizing regular properties like homogeneity and isotropy. Symmetry in Euclidean space is represented by translation and
rotation group transformations. A discrete graph, on the other hand, has too little structure to allow uniformity to form long range order. Nonetheless, there are processes in graphs that render dissimilar actions
symmetrical. These include process equivalence, redundancy of nodes and paths (also called degeneracy
in statistical physics), as well as the existence of ‘fixed points’ or absorbing regions of the graph that
transmute any values into a ‘zero’ value. The semantics of how we deal with these are related to the
efforts to give meaning to division by zero in rings and fields [20] and we’ll comment on these at the end.
Any kind of process from start to finish is basically isomorphic to some kind of journey in some kind
of space measure by some kind of time. Time in this sense is the Aristotelian concept of proper time
as countable changes, as observed by the agent concerned. These ideas have been discussed at length
in a variety of works [21]. The virtual or imagined space of concepts has no obvious structure of its
own. It can be imagined in any shape we like. For example, any neural associative memory leads to
the existence of ‘hubs’ or ‘appointed nodes’ that join together many others in different ways because
of shared attributes or common containers. Without the ability to express semantic symmetries, such as
belonging to the same class or type of information, it would be impossible to reduce the entropy of graphs
and create orderly information.
In any knowledge representation, which is neither complete nor undirected graph, there will be hubs
that lead to absorbing states and information loss under certain transformations. The effect is to erase
information from the interior states of the nodes, which can only be replaced with new boundary data from
outside the graph, such as outside policy choices. Absorbing states are non-conserving of information.

2

Semantics of spacetime and spacetime semantics

We define the Semantic Spacetime model first in terms of the kinds of links that connect nodes, and then
in terms of the kinds of nodes that make sense with those connections.
In a process view of the world, each point is a process event that needs to be justified rather than
postulated as a generalized container for motion. This view is behind the famous relationship between
Feynman diagrams and the algebraic and differential generating functional representations of Schwinger

5

and Tomonaga in Quantum Field Theory [22–24]. Further, both in Quantum Field Theory and in Group
Theory one has ladder stepping operators (creation and annihilation) generating graphs by the action of an
algebra of operators over abstract states. Underpinning all those variants are a mathematical description of
rings and fields that underpin everything else practically as axioms. Questioning these feels unproductive,
yet graph representations in particular have interpretations of arithmetic that poke holes in the rules we
take for granted. One has to be clear about whether returning to the same state is actually the same node
in a graph or a new version of a corresponding node which is similar but distinguishable. These notions
have been long understood in statistical mechanics and thermodynamics [25]. In this regard, a semantic
spacetime is akin to causal sets [26–28].

2.1

The purpose of Semantic Spacetime

Semantic Spacetime can be used in a number of areas including agent modelling, collaboration networks
like supply chains, service mapping, formal reasoning, and narrative representations. The role of a knowledge representation is to capture experiences and to abstract and generalize them, associate and collect
them into meaningful buckets that makes the knowledge easy to access and interpret. For example, the
flow of processes through a general graph can represent:
• Scene description and forensic reconstruction.
• Flow of utilities (e.g. electrical, water) through grids etc.
• Communications networks like the telephone and Internet networks.
• Dependency networks, like supply chains, ownership and market structures.
Any amount of detail can be added to a graph of meaning, in principle. Recent developments in
Large Language Models has placed much emphasis on generating fluent language, which is a vast area
of subtle meanings. Before we get as far as semiotics and linguistic considerations, we need to look into
the constraints on meaning imposed by topology and conservation of information. Dynamical considerations will always trump semantic considerations, because flow is a dynamical phenomenon semantics
piggyback on quantitative amounts.

2.2

The 4 relationship types

The Semantic Spacetime model is basically a graph structure in which nodes and links are used to represent and expose the compressed meaning normally described in more fluent natural language. This is in
the spirit of formal languages and state machines; however, it operates on a coarse level from the assumption that anything that happens is a process than can be expressed in terms of some kind of spacetime
journey.
The four types arise from the model of agents. In an agent model of causality, which is fully localized, processes and information only happen inside agents unless the agents cooperate to represent larger
structures. However, it is clear that activity only happens by agent states changing. Ideas and structure
cannot be imposed upon them from outside without violating their local autonomy. This is a strong form
of locality as expressed in physics. Based on the semantics of agents, and the model of intent developed
by Promise Theory, the semantic spacetime model settles on four basic arrows between nodes in a graph
that are postulated to be sufficient for any semantic description [1–3]. This remains a hypothesis for now,
but it is not a particularly original one. Various authors have suggested that spacetime concepts underpin natural language, on the basis that they are the only objective concepts an organism has to bootstrap
meaning from. Promise Theoretically, one derives four kinds of promise or relation between nodes in
order to represent spacetime processes. These are called (see table 1):
6

ST T YPE
ST 0 = ‘N ’
‘NEAR’

ST 1 = ‘L’
‘LEADS TO’

ST 2 = ‘C’
‘CONTAINS’
ST 3 = ‘E’
‘EXPRESS
PROPERTY’

F ORWARD
is close to
is similar to
sounds like
is correlated with
enables
causes
precedes
to the left of
contains
surrounds
generalizes
has name or value
has property
expresses attribute
promises
has approximation

R EVERSE
is close to
is similar to
sounds like to
is correlated with
depends on
is caused by
follows
to the right of
is a part of / occupies
inside
is an aspect of / exemplifies
is the value of property
is a property of
is an attribute expressed by

S PACETIME STRUCTURE
PROXIMITY
“near”
Semantic symmetrization
similarity
ordering
GRADIENT/DIRECTION
“follows”
boundary perimeter
AGGREGATE / MEMBERSHIP
“contains” / coarse graining
qualitative attribute
DISTINGUISHABILITY
“expresses”
Asymmetrizer

approximates

Table 1: Examples of the four irreducible association types, characterized by their spacetime origins, from [3]. In
a graph representation, ‘has attribute’ and ‘contains’ are clearly not independent, so implementation details can still
compress the number of types.

• 0 NEAR: a directionless assertion of equivalence or proximity between two nodes
• ±1 LEADS TO: a temporally or causally ordered arrow denoting a sequence of events
• ±2 CONTAINS: a spatially ordered collection of containment regions
• ±3 EXPRESSES: a locally promised attribute of a node or distinguishing mark
The directionless type 0 (proximity) may be interpreted as an equivalence, approximately equal to, close
to, etc. Type 1 (linear sequential order) may represent time, or unidirectional ordering, causation, dependency, etc. Type 2 (containment) may represent membership in a group, generalization of a collection of
concepts, location inside or outside a perimeter, etc.
See table 1, which originally appeared in earlier papers [3, 4] with some errors. An alternative way
of listing the 4 types is in a tabular form (see table 2). Some of the relations address physical material
attributes, while others represent virtual or conceptual attributes used to describe scenarios.
• Physical is also assumed to refer to properties that are external to the agent, and are therefore
‘tangible*, ‘material’, or even ‘real’.
• Virtual is assumed to refer to properties that are internal to the agent (or ‘somewhere else’ in a
hidden dimension), and are therefore intangible or purely informational.
Any number of aliases or alternative interpretations of the four spatial relationships are possible;
indeed, they are encouraged in effective communication for expressivity and qualification. However, the
more specialized kinds of arrow we introduce, the harder it is to reason about them directly. However, we
should also be cautious that informal association of linguistic metaphor also leads to confusions about the
appropriate classification of meaning under the irreducible types, as interpretation by metaphor is fluid in
human language [15]. What the four types enable is a basic generic form of reasoning about the meaning
of a graph based on what kind of subject it describes.
7

DISTINCTION INFO

EQUIVALENCE

DISCRIMINATION

( PHYSICAL ) SITUATION

CONTAINS

LEADS - TO

spatial

temporal

NEAR

EXPRESS PROPERTY

similarity/distance

attribute/name

( VIRTUAL ) INFORMATION

Table 2: A alternative tabulation of the arrow types as belonging to physical (exterior) and virtual (interior) realms.
Expressions may either denote equivalence of neighbours or group roles, or discriminants of nodes or what forms
sequences.

ROLE / MEANING

NODE TYPE

PROPERTIES

space/direction

ACTIVITY / ACTION

event

ephemeral/realized

timelike (process) agent

SUBJECT / OBJECT

thing

invariant/realized

spacelike (snapshot) agent

SUBJECT / OBJECT

concept

persistent/unrealized

virtual (role/intention) agent

Table 3: Agent node types in semantic spacetime. Virtual agents may be thought of as being situated or ‘interior’
attributes of spacelike or timelike agents.

2.3

Resolving link type ambiguities with 3 types: events, things, and concepts

There is potentially some freedom in how to represent information given the foregoing link types. Even
with the four SST categories there are some potential modelling ambiguities, so from a computational
perspective it’s helpful to eliminate these with an additional formality. The residual ambiguity can be
resolved by recognizing nodes as belonging to one of three meta-types, denoted with small Latin letters
(e,t,c), which are induced by the four link types. The result is a generic γ(3, 4) representation, where:
e Events (e) An event is a temporary or ephemeral phenomenon. Such happenings may persist or
change in time, by L EADS - TO which is a time like transition vector.
t Things (t) are persistent phenomena, physical or realized (manifest or reified) agents in what one
would call classical space. A thing is a persistent phenomenon but may be created or destroyed.
They are agents that behave like matter.
c Concepts (c) A concept is an invariant notion which cannot be created or destroyed. It belongs to a
virtual space of ‘unrealized’ or ‘potential’ characteristics that can only be materialized by attaching
to a physical material agent.

2.4

The combined Γ(N, L) 7→ γ(3, 4) representation

With the selections described above, we have a compact set of organizing meta-types that allow certain
basic inferences to be made, relating to process semantics. The typing immediate implies a few constraints
8

that tighten up modelling. We can summarize the design implications for these choices in a few rules.
1. Things may be contained but not expressed.
2. Concepts may be expressed but not contained.
3. Concepts become realized by anchoring them to things or events.
4. Verbs are dangling concepts without a subject or object to instantiate them.
5. Verbs that are anchored to subjects or objects (things) are events.
6. A state of being which is realized is an event (john is/was happy, there was a moment of happiness).
7. A state of being which is unrealized is a concept (happiness).
8. A type of thing which is realized is a thing (a specific animal).
9. A type of thing which is unrealized is a concept (a general animal).
Example 1 (e, t, c examples) We shouldn’t think of objects as having a single representation in a knowledge space. Different aspects of a phenomenon have to be represented differently to obtain functional
semantics:
• Mark’s life is an event.
• Mark’ body is a thing.
• Mark’s character is a concept.
• Mark’s appearance is a series of events expressing observations of descriptive concepts.
Example 2 (What is real?) Which part of semantic spacetime does an idea belong to? Physical or virtual? In the debates over the meaning of quantum physics, many authors have questioned what ‘being
real’ means, e.g. whether we can say electrons are real if they don’t behave like large scale material
bodies. The use of ‘real’ is too loaded with ideological connotations to be useful. Adopting this terminology of γ(3, 4) avoids these ambiguous stumbling blocks. One can be more precise about what is ‘real’,
‘beable’, or conceptual etc.
Example 3 (Collectives or collective histories?) Our ability to group events and concepts into a single
namable entity “all bicycles” (a concept) from the evidence of many bicycle instances (things) is powerful
but misleading because it exists only at a snapshot in time. In general, a process or history is a storyline,
a conical structure of causal development rather than an encapsulation of things. We only reduce it to
a single node in order to give it a proper name. Proper naming induces “nodality”, but generalizations
may be time dependent.
In the spacetime sense, things or concepts can be invariants. Concepts may be said to exist yet be unrealized, i.e. they exist in the imagination, as a model or idea. Events are ephemeral but still realized by their
happening. The distinction between ideas that are ‘realized’ and ‘unrealized’ is a simple clarification of
the implicit metaphorical indirection we use all too frequently in natural language. Are we talking about
the actual object or event X or are we talking about the idea of X in some context, or all possible X?
Invariants can’t be contained except by other invariants
Example 4 (Many roles and contexts need different types) Consider the word ‘library’, which we might
use quite liberally in language:
9

• A library is a concept.
• The library is a thing.
• Library is a role for a building.
• Library is a location attribute for an event.
• The opening of the library is an event.
• A library contains books is a statement of concept.
• The library contains books is a statement of things.
• Library expresses attributes, old, well stocked, damp, at centre of city.
When there are multiple paths, the selection of a particular value in a multi-valued inverse is affected by
whatever semantics are applied to the arrows.

2.5

Strategy in semantic graph representation

Graphs as knowledge representations are typically understood either as classifications or ordered concepts
or as social networks:
• Taxonomy: a hierarchy of proper naming and sub-classification.
• Ontology: a network of proper naming, classification and rules.
• Ad hoc property graphs: an unstructured network of atomic items and their relationships. All items
are assumed to describe entities on a similar scale or level.
In recent years, the proliferation of graph models and Semantic Web technologies has encouraged users
to model nodes in a graph by arbitrary linguistic atoms. Any simple conceptual meanings are presumed
‘atomic’. The Semantic Web is best known example of a social network in the stigmergic sense, with
embedded order [6, 29, 30].
Nodes are given meaning with supplementary ‘ontologies’ [14]. The ontology movement is rooted
in the primacy of first order logic for computer scientists. The focus of an ontology is to specify and
share meaning, whereas the focus for a database schema is to describe data [31]. A relational database
schema has a single purpose: to structure a set of instances for efficient storage and querying. The
structure is specified as tables and columns. An ontology can also be used to structure a set of instances
in a database. However, the instances would usually be represented in a [possibly virtual] triple store,
or deductive database rather than directly in a relational database. Ontology acts as a formal straitjacket
because it only works if everyone agrees to it.
Trying to naively shoehorn natural language into a graph structure is unproductive. For example,
suppose we try to express In common speech, we regularly transmute different aspects of a description
into one another all the time without much caution. Linguistic inference makes the algebraic underpinning of natural language grammar difficult. The brain skips many pedantic typological conversion steps,
flattening out sequences as if coarse-graining steps by aggregation. For example, consider:
The computer (saves data to) disk.

(1)

This is the kind of relation one often sees in naive triple store models. The arrow ‘saves data to’ is bursting
with assumptions. It is so specific that it is unusable outside of this example. In terms of the γ(3, 4) types
we can compare it to an SST graph (see figure 1).
10

?

?

?

saves data

disk

computer
event
thing

thing

saving event

computer

disk

start location
end location
has code

contains

thing

executing
code

has content

disk sector
location

computer
program

expresses
value

data
concept

Figure 1: A triplet graph compared to an γ(3, 4) graph.

One could argue that the ‘data’ implicit in the upper link of figure 1 should be provided as an input to
the graph. Without a decomposition, this can only be done by extending notion of links to hyperlinks as
in Milner’s bi-graphs [1, 32]. However, this introduces basic ambiguities that are easily resolved by the
γ(3, 4) decomposition.
The dotted lines indicate the subgraph representing the triple link. We see that it is not a simple
substitution.
If we say a thing smells of perfume we are adding an implicit indirection from the concept of the smell
of perfume to the liquid perfume itself. A smell is not a liquid, but there is only only natural interpretation
of this muddling of proper names so no harm is done.

The cake (looks like) mark’s house

(2)

What type is “looks like”? The cake is clearly a thing. Mark’s house is a thing, but “looks like”
expresses a similarity. If we write “has the appearance of” is looks more like a concept attribute, which
makes more sense. However now we have the rule that only concepts (not things) can be expressed as
attributes. Mark’s house is not hanging off the cake, only the likeness or image of mark’s house, which is
a conceptual representation.
These considerations are pedantic, like formal logic, because these are the typological distinctions that
lead to precision. But then why not simply use logic? We are trying to avoid that level of specification by
having only four types.
Concepts may express other concepts.
Stubbornness (means) explanatory concept
A hammer (may be used for) hammering event/activity
” (may be used for) hammering activity

11

(3)

An example of something is an event or instance not a member of a group
How shall we decide the correct kind of link? When do we use express versus contains? The meanings
of these concepts are rooted in spacetime ideas.
London (makes) cars
Tractors (are supplied by) Massey Ferguson
Apples (have colour) green
Apple (is a) fruit
In each of these cases, the kind of objects connected are wildly inhomogeneous. Without semantic homogeneity there cannot be homogeneous reasoning. Every new arrow relation. It’s not difficult to see
that many implicit steps are involved in each of these. Although we might say these abbreviated forms,
we imply many more steps. Clear the city of London does not make cars: some factory located there
contains a process of manufacturing and assembling. Making cars is a property of the factory. However,
as the container of the factory, the city somehow inherits that promise of making cars. On the other hand,
Sally is stubborn and Sally lives in London. This does not mean that London is stubborn.
How can we resolve these idiosyncratic anomalies in the reasoning? The key to unravelling these
issues is to homogenize phenomena as generic processes. What process transmutes cities into cars, machinery into companies, fruit types into colours, and fruits into fruit types? The specific relations are not
as important as whether something is a chain of events, an explanation of an attribute, or a part of a larger
whole.
If several nodes are joined by a common association, typically a property expression or group container membership, then we can infer an implicit equivalence through correlation to the third party. If we
have
A (is located at) X
B (is located at) X

(4)

we can assume that A and B are correlated somehow. However, this is likely lazy modelling. Is this really
an invariant fact, or merely an ephemeral meeting? The database is describing a misleading snapshot of
reality. Most likely A and B only met in a single event of a more dynamical scenario. X should not be a
location but an event.
John Williams (composed) Star Wars

(5)

The use of a verb ‘composed’ lead to an ambiguous typing. Is Star Wars the film, the soundtrack, the
brand, the franchise, etc. Is the composition a script, a piece of music, a clay model? Human inference
can make a good guess and derive meaning from this (perhaps not without some confusion in general),
but a machine algorithm relies on precise typing and cannot.
One could write explicitly
John Williams (composer has musical composition) Star Wars Soundtrack

(6)

This is then a reusable relation that implies two clear types: composer and musical composition Written
in this way as an invariant factoid, it misses the opportunity to be precise. We should, once again, turn to
events as specific instances (analogous to objects instantiated from classes in computering).
John Williams

composed the music for the movie Star Wars

”

(event has composer) John Williams

”

(event has composition) Star Wars Soundtrack
12

(7)

If we write the link more explicitly:
Star Wars the movie (has soundtrack) Star Wars Soundtrack

(8)

we notice that the link itself induces types between the end points as long as we choose link names
accordingly. Thus the explicit logical typing of objects is an unnecessary constraint as long as there is
broad consistency of naming. This simplifies the matter of search by random access when we are unclear
what we’re looking for.
event
then

get the party started

investigation
begins

then

Professor Plum murders Miss Scarlet
in the library with the bread knife
because she refused to marry him
example of

concept

involves

Professor Plum

involves
contains

murder by subtle knife

involves
event

concept
answers question

how

example of

Prof Plum & Miss Scarlett in the library

event

subtle knife

Miss Scarlet refuses to marry Professor Plum

kind of

used for

In the library

concept

cutting bread

concept

concept

murder

used for

knife

answers question

event

involves

where

cutting interdimensional rifts

refusal marriage

answers question

concept
concept
concept

involves

involves

involves

thing

concept

Miss Scarlet

involves

use of subtle knife

library

marriage
concept

what action
concept

why
concept

Figure 2: Describing a more complicated scene in terms of pure spacetime semantics is easier if we clearly describe
events as more than the casual abbreviations we use in language. This is analogous to the distinction between a class
or datatype in computer programming and a particular object instance of that class.

We use the name of an object and the concepts and events it is part of loosely in natural language. Indeed, this is one of the strengths of natural language, which is likely optimized for the implicit functional
capabilities human brain: many such connections can be left implicit and be filled in by inference. Part
of the goal of semantic spacetime is to help understand these matters.
How we handle a complex issue like ownership may be seen in figure 3. Ownership is a mixture of
concepts and things. The person ‘mark’ is a thing so can only express a concept. To form a collection
of things we need an entity that can contain them under a single umbrella. The estate of mark is thus a
thing, and in order to express the ownership, we need to express the concept of ownership as a property
of this collection. Since mark cannot (in a natural since) contain many other items that are possibly far
away from mark, we use the attribute of connecting ownership as an expression of mark’s identity (which
is a concept). Although this seems like a long winded way of expressing the concept, it has the virtue of
adhering to the simple rules of the algebra, and removing the ambiguities about how to define the concept
of ownership. Ownership is decoupled from the constraint of being a physical part of something else, but
it still can be.

13

refusal

thing
concept

mark
person

+E

mark’s things
+C

mark in role
of owner(id)

+E
+C
guitar

+C
goldfish
castle

Figure 3: We use the name of an object and the concepts and events it is part of loosely in natural language.
Ownership is a mixture of concepts and things. The person ‘mark’ is a thing so can only express a concept. To form
a collection of things we need an entity that can contain them under a single umbrella. The estate of mark is thus
a thing, and in order to express the ownership, we need to express the concept of ownership as a property of this
collection. Since mark cannot (in a natural since) contain many other items that are possibly far away from mark, we
use the attribute of connecting ownership as an expression of mark’s identity (which is a concept).

2.6

Directional inference in the γ(3, 4) representation

Spacetime operations are often associated with group transformations that form chains or closures. With
a concept of graphical scale, and the type separations of γ(3, 4), we can now attempt to make certain
inferences without any specific ontological rules.
The aspect of systems which is supported by Semantic Spacetime is the frequently neglected dual
importance of quantitative dynamics with qualitative semantics [33]. Process representations that encode
knowledge may have several interpretations. The most common understanding of computation is based
on the arithmetic conventions of rings and fields for addition and multiplication of real numbers. Our
universal adoption of standard conventions means that we rarely question to consistency or meaning of
these rules, yet they are fraught with many ambiguities, which normally only surface when faced with the
‘loose end of’ division by zero.
Our semantic model is relevant here too, as representations of computations may involve either physical interlinking of agents (e.g. agents collaborating on a chip die to perform a computation) or purely
informational maps recording conceptual models of the processes (e.g. symbolic algebras).
The fact that we can represent computations as process graphs should be no surprise. It is a special
case of expressing reasoning as a graph, from ad hoc stories ‘once upon a time’ to highly constrained
algebraic logics. The lesson, however, is that this is not as simple as chaining together arbitrary triplets
of named storage locations as in an RDF graph. The usual formalized notion of an ontology is of little
use too, since it is too specialized and once always needs to extend or replace it. An ontology can never
stabilize except by limiting data or constraining the allowed phenomena.
The types C, E do not ‘propagate’ indefinitely in the sense that L does. They are representative of the
snapshot one obtains by freezing temporal evolution in a fixed configuration. We thus need to investigate
the idea of scaling for inference. For instance, if we take an example and generalize or specialize it
(going ‘up’ in the CONTAINS or EXPRESS direction), does the generalization have the same properties as
the example? For example:
• Mark is human.

14

• Mark is tired.
• All humans are tired.
This attempted syllogism is clearly wrong, but the opposite direction may be true.
• All humans are annoying.
• Mark is human.
• Mark is annoying.
As pointed out by Couch [34, 35], the approximate notion of “might be true” or possibility links
is sometimes the best one can do when reasoning, There is no precise logical one hopes with formal
ontology. For example:
• If A contains B and A contains C, then B and C might be near one another on the scale of A.
• If an event E involves A and B, then A must have been near B on the occasion of that event.
• If A contains a collection nodes Bi should they inherit properties expressed by A?
• If a collection of nodes Bi are embedded in A → Bi → C, then all the Bi are symmetrical or
equivalent with respect to this particular process and may be considered part of a single supernode
with redundant elements.
Interpretations like these are involved in the processes of reasoning by deduction, induction, and
abduction, etc. The application of these ideas to unlabelled probabilistic transitions of Markov chains is
the way current diffusion models of machine learning attempt to ‘reason’ [18].

3

Graph-algebraic Semantic Spacetime

Based on the foregoing definition of nodes and links, we can say more about the properties of these
graphs. A graph may be represented structurally by a number of matrices, in particular the adjacency and
incidence matrices, which represent maps of locations and flow gradients for the process concerned.

3.1

Proper names as semantic coordinates

In knowledge systems, such as taxonomies, one attempts to give a unique name to concepts in a contextually appropriate way. The name, although intended as a true representation of its unique meaning is
often not unique in practice. In a coordinate system covering a region of space, in which one attempts
to label distinct locations, a proper name is a kind of semantic coordinate. Unlike numerical coordinate
systems, proper names are often multivalued, which leads both to opportunities and problems when reasoning. For a simple graph representation in which the characteristics of the node are the name itself,
name associations may be considered tautologies, since our ability to make distinctions relies on there
being observable differences.
The scaling of names to groups and regions is not altogether trivial, however. In statistical subjects,
the importance of node or entity distinguishability has long been known and was shown by Boltzmann
to be associated with the degree to which systems are able to exert a causal influence (free energy and
entropy concepts) [25]. This issue of unique identity affects the way we model concepts and things in a
graph, particulary when using spacetime as a model.
The concept of proper name interacts with the γ(3, 4) types. A graph node is associated with a distinct
identity. The numerical identifier of the node or semantic identifier may be associated with its ‘proper
15

name’, i.e. the collection of symbolic or numerical attributes that are expressed within or outside the
node, i.e. the union of interior attributes S and the set of links {E(ni )} of type ‘EXPRESS PROPERTY’. In
semantic spacetime, scaling allows us to take an entire book of text as a node in a graph. The proper name
of the node is the entire text of the book. The node can be decomposed into smaller parts in a variety of
ways to express the meaning of the entity.
• A book, considered as an entity, has its entire text as its unique identifier or proper name. We can
give it several aliases, such as a title, a cryptohash or an ISBN number.
• The concept of the book with its unique text is realized in many physical copies, which express
the concept of the book by reproduction. The physical books exists and can participate in different
events (see figure 6).
• Compressed descriptions of events, things, and concepts can be unified under the umbrella of a
name: ‘The Battle Of Britain’, ‘Tractor’, ‘hunger’. As patterns, we define names to be concepts or
virtual attributes, rather than physical realizations. Names are usually recognizable patterns, with a
variety of manifestations: in speech or writing, etc. It’s the content of the realizations that imbues
the name, not the mode of implementation.

3.2

Graph structures: appointments and loops

When several nodes point to a single node as their successor, we call that an appointed node [5]. This
leads to a local amplification of flow into the node. Nodes that are pointed to are also called ‘hubs’, while
nodes that point to many are also called ‘authorities’ in social graphs [36, 37]. An appointing node is the
equivalent predecessor or source for several successor nodes. This is a division of the flow through the
node into weighted distribution, according to the link weights. Appointing and appointed nodes correlate
their appointees implicitly, and thus form a common dependency in reasoning. Such nodes are important
for several reasons. Pragmatically, they a absorbing nodes and therefore lead to division by zero issues.
Graphs may contain structures that have both semantic and dynamical consequences.
• Sources and Sinks: these are nodes that start and end a path through the graph. They exchange
places if one changes the sign of the link type.
• Appointed nodes: when several nodes point to a single hub that appointee is called an appointed
agent in Promise Theory. The cluster of nodes all pointing / electing a single individual are thus
correlated by the appointee (they have it in common). Such structures help us to see processes and
process histories.
• For ”leads to” arrows, these structures are confluences of arrows or explosions from a point.
• For ”contains” arrows, these structures are the containers or shared members
• For ”property expression” arrows, these structures are compositions of attributes or shared attributes common to several compositions
• For ”near” arrows, these structures are synonym / alias / or density clusters
Appointed nodes that are themselves appointed recursively form nodes that are called ‘central’.
• A central node for ‘leads to’ has a high level of involvement, implying a high mass for flows.
• A central node for ‘contained by’ arrows is the container.
−CONTAINS

{ni } −−−−−−→ n
16

(9)

• A central node for ‘contains’ is a member of several containers (a member of many categories).
• A central node for expressing a property is a widely shared or common property.
• A central node for originating properties has a rich spectrum of attributes.
• A central node for being similar or near others implies a high density or a high level of redundancy
in interpretation, perhaps a popular concept.
If we don’t recognize these distinctions etc , the four link classes or types run into ambiguities when
trying to classify the geometry of arbitrary semantics. For instance, EXPRESS versus CONTAINS are
superficially similar, they both represent interior states, however one refers to the physical materialized
makeup of a thing while the other refers to a concept.

3.3

Propagation and terminating absorption of the γ(3, 4) types

A node that propagates may form chains. Nodes that ‘absorb’ arrows are natural endpoints of graph flows.
Events are naturally propagating, without any necessary end. Both concepts and things are absorbing
types of node because there is a most primitive attribute in practice.
The mapping between directed graphs and sequences of events (as ST-1 ‘leads to’ arrows) creates an
obvious geometry for processes in a graph. These arrows become associated with proper evolution of
states, e.g. Hamiltonian evolution in symplectic systems.
Consider examples for how each of the 4 types propagates.
• A terminating L chain sequence is one in which there is no natural followup in the narrative:
(gestates into)

(becomes)

(flies to)

egg −−−−−−−→ caterpillar −−−−−→ a butterfly −−−−→ tree

(10)

Notice that ‘a butterfly’ is really the event of changing into the state of being a butterfly, which is
an event, since it wasn’t a butterfly before. Similarly ‘tree’ is really a shorthand for ‘the visitation
of the butterfly to the tree’, which depends on the material thing ‘tree’, but is not the tree. These
language subtleties trip modellers up frequently. The event may refer to a thing, but it is not the
same as the thing. If we neglect to make these distinctions, and believe too literally the meaning
of our abbreviations, we fall into the inconsistent the semantics of many knowledge graphs. Leads
to chains have no obvious end, unless we choose to restrict them, e.g. to focus on a particular
butterfly that eventually dies or transforms back into its constituents. Ideally, we would write the
events more clearly (see figure 2),
• A C chain:
(owns)

(made of)

(contains)

Mark −−−→ car −−−−−→ atoms −−−−−→ quarks

(11)

• An E (or P ) chain:
(has prop)

(has prop)

(has prop)

(has frequency)

(has units)

diagram −−−−−→ visual −−−−−→ colour −−−−−→ blue −−−−−−−−→ f −−−−−→ Hz

(12)

• An Nt chain:
(looks like)

(sometimes confused with)

(resembles)

Horsefly −−−−−−→ a butterfly −−−−−−−−−−−−−→ a moth −−−−−−→ angel.

17

(13)

The types of things now enter into these chains of reasoning.
The metaphorical text of the song ‘Love is Like a Butterfly’ can clearly be written as a simple triplet:
Love (is like) Butterfly. However, this doesn’t quite work in this more stringent representation:
(is like)

Love −
̸ −−−→ a butterfly,

(14)

because love is a concept and butterfly is a thing, and these cannot be alike while retaining this type
distinction. Cleary natural language works by metaphor much of the time, rather than by logic, and our
brains are quite good at seeing through these inferences. This, on the other hand, makes the challenge
of more careful logical explanation from natural language a non-trivial challenge. Arguments may be
presented for keeping or eliminating metaphor in graphical knowledge representations. As long as we
want a more formal calculative framework for reasoning (which is typically what we ask of machines),
the shortcuts of natural language are probably best avoided unless we can find a method for replacing
simplistic arrows with subgraphs that may be substituted in their place. In a sense, this is what Artificial
Neural Network representations are doing on a probabilistic level.

e
+

t

+
+

c

Figure 4: Allowed semantic transitions between node types, by kinds of arrow. There is a separation between virtual
or conceptual states and physical or material characteristics. Events are the encapsulating class for both of those.

3.4

Allowed type transitions of γ(3, 4)

We can now summarize the algebra of rules for valid graphs in the γ(3, 4) representation. See figure 4
and table 4.
Due to linguistic inference, once again, it seems that an event ought to be able to express or refer to
another event as an attribute, e.g.
Party celebrating the Olympics (refers to) the Olympics

(15)

The Olympics was an event and the party is an event. However, if we seek a clean distinction we need
a more pedantic eye. The use of Olympics in this case refers to the collective memory of the event, not
the actual happening event itself–which is a concept without physical manifestation. We might prefer
to mention Olympics only once in a knowledge graph, but then we would render all uses of the word
equivalent, which is semantic nonsense. The same type of reasoning can be applied to explain why things
cannot be expressed as properties. For concepts that seem to express things, e.g.
fast food hatred (is about) fast food,

(16)

we observe that the reference to ‘fast food’ is not a reference to an instance of fast food, but rather a
reference to the whole class of things we call fast food, which is a concept. Our minds quickly create
short cuts through these matters, yet we are also intuitively aware of the distinctions. This underlines
18

T RANSITION
e (±L) e
e (±C) e
e (Ne ) e
e (+C) t
e (+E) c
e (+E) c
t (−C) e
t (±C) t
t (+E) c
t (Nt ) t
c (+E) e
c (−E) e
c (−E) t
c (E) c
c (Nc ) c

E XPLANATION ( EXAMPLE )
An event can be followed by or lead to another event
An event can contain or be part of another event
An event can be similar to another event by any criterion
An event as a region of spacetime can contain a thing for its duration
An event can express a property or concept (timestamp)
can event can express a property or another event (celebration of Christmas of 74)
A thing can be part of an event, but an event cannot be part of a thing.
A thing can contain or be part of another thing
A thing can express a concept as an attribute (blue car)
A thing can be close to or like another thing
A concept can refer to an event as an attribute (that one time at band camp)
A concept can be an attribute of an event (a time of happiness)
Concepts can only be attributes expressed by things (blue car)
A concept can have properties or be a property of something else
e.g. (blue is a colour)
A concept can be similar to another concept, (aquamarine,turquoise)

Table 4: Explicit transitions allowed for events, things, and concepts through the four link meta-types.
(as is well appreciated) that, without quite sophisticated automated analysis capabilities, natural language
could not be understood literally. Our penchant for metaphor is busily at work in natural language [15].
The consistency of these relations can be show using the matrix algebra in section 4. In short, we can
use this decomposition to define what are events, things, concepts. One could add more detail, but we are
trying to compress description into simple elements rather than exfoliating.

L
C
E
Ne

e

C
E
Nt

t

L
C
E
Ne
C
E
Nt

E
c
Nc

E
Nc

Figure 5: Allowed semantic transitions through node types, by kinds of arrow. Not all of the links are freely joinable,
however, so there are restrictions on allowed transitions.

3.5

Inference rules and symmetries

In an unconstrained graph there can be no rules for inference, because rules require some regularity and
functional predictability. In Semantic Spacetime, however, one has four broad kinds of relation and three
kinds of node or entity.
19

If we seek a strongly constrained deterministic logic, as in ontology approaches to graphs, the result
will be either simplistic or intractable. Inference about common properties are straightforward, though
remain speculative.
• If a collection of nodes ni are contained by a node nC which expresses property P , then one might
infer that the contained nodes might also have this property.
• If a node has a property P and is declared to be similar/near another node, then the similar node
might also have this property.
This possible inferential reasoning goes back to the discovery by Alva Couch in join work [34, 35],
which was subsequently deepened in the development of semantic spacetime. Unlike logical ontological
schemas, this kind of reasoning is simpler but inexact. With ontological first order logic, results are either
precise or non-existent. In practice, most relations on data that are not carefully designed will fail to yield
any result due to the over-constrained nature of first order logic.
A second kind of symmetry concerns patterns of inference in the linkage of nodes. Duplicate nodes
may arise in a graph either by accident or by a deliberate encoding of redundancy (degeneracy). If a
collection of nodes each possesses the same incoming and outgoing links of a given ST-type, then we
can infer that they are functionally equivalent with respect to that process. This implies that they can be
treated as a single node (which we refer to as a supernode) for those intents and purposes, though perhaps
not all. While the nodes might have the same links for, say, causal trajectory (‘leads to’), they might have
different properties. This suggests another possible set of inferences or warnings to flag: why is there a
partial but not complete equivalence? Is it intentional or accidental. Is it, in fact, an error?

3.6

Absorption by blind alley and by statistical aggregation: scales and degrees
of freedom

How do we draw a ring around a region of interest or influence in a graph? The role of an agent boundary
in determining what is a value or operation in a given context is important to many of the concepts as
we scale up or down a hierarchy of meaning. Concepts emerge by recombination of atomic or genetic
concepts and attributes, suggesting that C, E spatial types have semantic limits. L may have frequently
have practical limits due to the ephemeral nature of interactions and processes in general, but there is no
obvious upper limit to the extent of time.
The structure implies by the SST γ(3, 4) model is of a graph composed of snaking causal sequences of
events or ST-type 1 (‘leads-to’) connected nodes, where each node has ‘contains’ and ‘express property’
nodes in orbit around them, expressing their interior attributes. These may themselves be in orbit around
nodes that contain or express them. A few ‘nearness’ links provide shortcuts (wormholes) between nodes
that are marked as being similar for reasons outside the scope of explicit causal process knowledge.
One can say that events effectively act as bipartite bridging nodes between invariant things and concepts, leading to a temporary connection in the sense that an event has a finite contextual validity or
lifetime even if the historical node remains in the graph.
Aborbing regions arise whenever there are source nodes or dead end sinks in a directed graph. Once
we descend into the details of a node, by CONTAINS or EXPRESS, we find their invariant attributes. They
live in an interior subspace orthogonal to the timeline of events. The timelike ST-type 1 (‘leads-to’)
are analogous to the Hamiltonian evolution in symplectic systems of physics. This orthogonal subspace
is analogous to the hidden dimensions of a Kaluza-Klein or string theory, a field space of forces or
‘interaction semantics’. The entire directional links for containment and expression point to ultimately
aborbing regions, as they are spacelike.
There is an erasure of information in two ways here:

20

• Flows that end up pooling at sink nodes are aggregated with a loss of distinguishable history or
identity.
• Processes that drive several nodes towards an appointed successor merge separate dynamical flows
rendering them indistinguishable. Conversely, a node that splits a flow to several successors with
a weighted distribution is either sharing (dividing) or amplifying (multiplying) the flow along the
links.
Clearly driving independent flows through a common node erases information and semantics, coarse
graining the result. If one cares to measure the incoming distribution against the outgoing, with an
entropy function, there is a change of entropy–potentially an increase or a reduction depending on how
one chooses to define the semantics of the result. One does not avoid process semantic issues merely by
adopting a conventional narrative.

event

event

event

storyline

thing
thing
thing

ideas about
thing

naming

Figure 6: The logical structure of events, things, and concept is subtle. Events happen in real exterior space. Things
exist in real exterior space, but ideas about things are interior to the agents that express them. They can become
shared by interaction (like entanglement of quantum agents) but they are a priori private on the interior of agents.

The significance of absorbing regions, sources and sinks, for a finite system is clear semantically.
Dynamically, however, there are different ways to interpret the processes.
There are certain elements that are, in a sense, atomic or irreducible by virtue of being a logical end
to a process. States with both incoming and outgoing arrows are transitory but reducible ultimately to the
sets of nodes either leading to it or emerging from it. In differential calculus one refers to these processes
as retarded and advanced solutions.
Equilibrium solutions are also possible by selecting from both sets of arrow, or removing the arrow
direction altogether. These are known as Feynman boundary conditions in physics. The latter is interesting because it introduces a second type of absorption for a process: a statistical absorption, which is
associated with entropy. Any bulk state of a system, which converges statistically by some separation
of scales into an average probability distribution of values, may be called statistically stable [17]. For
example, an ideal gas at finite temperature has a statistically stable (or maximum entropy) distribution of
particle velocities, which it makes no sense to count as distinguishable phenomena. This, at the semantic
level of the bulk gas, the thermal state is absorbing—and represents another kind of ‘zero’: placing a

21

small object into a large (fabled infinite) reservoir at temperature T0 leads to the process:
Equilibrium Tobject = T0 .

(17)

In other words, semantically, equilibration is another kind of zero operation.

4

Matrix representations of graphs

A graph has an associated matrix representation for mapping the topology onto a linear map acting on
rings or fields. The matrix representation is a crucialmap between arithemtic algeabraic rules and the
realization of processes as graphical structures.
The adjacency matrix A and its transpose AT are square matrices, whose rows and columns are
the node labels, and whose non-negative elements represent quantitative link weights. The numerical
values are typically set to 1 in elementary texts on graph theory, but they can have relative weights as
well as semantic labels. These may be chosen as non-negative real numbers. An undirected graph (with
arrows in both directions) is symmetrical about the leading diagonal, i.e. A = AT . A directed graph is
asymmetrical and its source and sink nodes, which are the starts and ends of paths, lead to zero rows and
columns, which consequently lead to zero eigenvalues of the matrix. This proves to be important in a
number of ways, as it implies the matrix is non-invertible, preventing predictive process-path reversals.
A complementary line or ‘join’ graph matrix is the complement of the adjacency matrix, where rows
and columns are represented by the links joining nodes. Both of these may be found from the incidence
matrix.
The incidence matrix and its complement describe the emission and absorption of link lines from and
to nodes. Since they are local to a single node, they may be associated with the promised intent of the
nodes concerned. The standard conventions in most texts are for undirected graphs and are unhelpful here.
Directed and labelled graphs with self-referential loops require a separation of the incident matrix into
two parts. These are analogous to the offer and acceptance promises in Promise Theory, and the matrix
elements factor from a Hadamard product form into two complementary matrices that are effectively the
square roots of the adjacency matrix: I (+) and I (−) .
Iˆ(+) Iˆ(−) = Â + Ĉ

(18)

for some diagonal matrix C belonging to the Cartan subalgebra of the flow. When there are no autoreferential (pumping) self-loops, C is proportional to the identity matrix. The incidence matrices I (+)
and I (−) correspond to the Promise Theoretic offer and acceptance promise rates [5]:

Iˆ(+)

=

L

e 1
t0
c 0
e
L
1
C 
1
E 
1
Ne 
1
Nt  0
Nc 0


Iˆ(−)

=

C
1
1
0
t
0
1
0
0
1
0

E
1
1
1

Ne
1
0
0

c

0
0

1

0

0
1

Nt
0
1
0

Nc

0
0 
1

(19)

(20)

Notice that these are not simple transverses of one another, as they would be in a simple undirected graph,
since a thing cannot be expressed (a forbidden state transition). We distinguish nearness links for each of
the three types e, t, c for convenience.
22

Rates of change within the processes can be represented as derivative. The ‘dynamical’ graph derivative for node values is defined ∇i vj = vi − vj . This corresponds to the usual Newtonian derivative
∂x v(x) for a function which is distributed over graph nodes ⃗v (N ). There is a second notion of rate of
change for a graph: because links define both direction and value between each pair of nodes, they also
behave as a vector field, which has a gradient role of its own. The matric of links mapping to links forms
a dual ‘line graph’ representation in which there are rows and columns for every independent link, no
matter the nodes they connect.

4.1

Matrices for γ(3, 4) skeleton

The γ(3, 4) skeleton graph can be represented, without explicit nodes only type names, as a set of matrices
characterizing the graph. Certain rules about process semantics mean that transitions between certain
node types are limited to specific kinds of arrow:
e
e ±L, ±C, ±E, Ne
t
−C
c
−E


Â = A(ni 7→ nj )

=

t
+C
±C, Nt
−E

c

+E
+E 
±E, Nc

(21)

This can be decomposed into a number of generators with antisymmetric (or anti-Hermitian) signatures:
e
e ±1
t 0
c
0

t c

0 0
0 0
0 0

e

e ±1
t  −1
c
0

t
1
±1
0

c

0
0
0

e
e ±1
t 0
c −1

t
0
0
−1

c

−1
+1 
±1



ÂL

ÂC

=

=



ÂE

ÂN

=

=

e

e 1e
t0
c 0

t
0
1t
0

c

0
0
1c

(22)

(23)

(24)

(25)

Conversely, arrows in a trajectory can only be joined by certain types of node as a path join matrix J:

Jγ(3,4)



a′


−
→ ni −→
a

=

L
C
E
N

L C
E
N

e
e
e
e
 e e, t e, t
e, t 


 e e, t e, t, c e, t, c 
e e, t e, t, c e, t, c


• A process L must terminate on a final event or never.
• A property attribute process E must terminate on an atomic concept (property).
23

(26)

• A containment process C must terminate on an atomic thing (component).
• A metric similarity process N need not terminate within the scope of connected nodes.
The key point for directed graphs, representing finite processes of different types, is the existence of
isolated states that absorb and emit transitions of the graph. In a ring or field this is not the usual case.
There one has ‘translational invariance’ or group transformations that are essentially unlimited, for both
addition and multiplication, with the important exception of the zero element, which is the one absorbing
state under multiplication.

4.2

Stable regions of a graph, information, emission and absorption

Particularly for directed graphs, but also for some undirected ones, an important feature within a graph
is the number of nodes at which the link flow converges (when absorbed by a node) or diverges (when
emitted by a node), i.e. for in- and out-degrees greater than 1. These are confluence and branching points
in the arrow vector field, which are literal and metaphorical singularities. As remarked in [38], emission
and absorption by nodes is associated with zero operations.
There are certain operators which transmute interior state to exterior movement in the graph. These are
related to ladder operators known in the context of differential equations, and they are further connected
to absorbing states of the graph. Convergent semantics with idempotence of an operation one some end
state comes about from ending up in a cycle that has the identity element as its final state. This is used to
good effect in enforcing policy choices [38]:
Ô⃗v

= ⃗v0

Ô⃗v0

= ⃗v0

(27)

Consider figure 7: We can write this algebraically as:

1
0
2

3
Figure 7: A convergence of flows at a point is an absorbing region of the graph, associated with a singularity.

Arrow n1

= n0

Arrow n2

= n0

Arrow n3

= n0 ,

(28)

which is clearly isomorphic to
0·1

=

0

0·2

=

0

0·3

=

0.

24

(29)

in other words, by virtue of ending up at the same state ‘0’, the arrows lead the process into a location
which does not remember the route by which is arrived there. If there are no arrows flowing away from
node 0 then the node is completely absorbing (a kind of black hole for the process), and it is called a sink
node. The adjoint, in which arrows are reversed would be a node that emits arrows from nowhere, called
a source. Even with arrows both incoming and outgoing, any such multi-line convergence is singular, and
indeed this is represented in the adjacency matrix of the graph by the existence of zero eigenvalues–and,
indeed, the implications for invertability of the zero operations. Inverse of zero is a topic that several
authors have discussed.
A hub operation of this kind turns a distribution into a single value and vice versa. Clearly, one can
turn n values into one by averaging or some selection process, but turning one value into n requires
more information. One can duplicate, triplicate, etc values to send out identically to multiple redundant
destinations. Then the distribution of values is unchanged. Alternatively, one can use a new source of
information in different directions to determine the result in different directions. One possibility is to use
a differentiated distribution of link weights. However one chooses to inject information, it has to come
from outside the starting node.
With multiple states distinguished semantically, rather than representing different locations on a neutral number line, we now have more responsibility to define the degrees of freedom carefully. In physics,
one uses entropy concepts to describe and measure the extent and homogeneity of a distribution over
states. The Shannon entropy [39,40] is defined over some distribution of states partitioned into N choices,
and measure each with an alphabet of states 1 . . . C.
X
pi = xi /
xi
(30)
i

S

= −

N
X

pi logC pi .

(31)

i=1

This is maximal S = logC N , when pi = 1/C, ∀i, and minimal S = 0 when pi = 1 for some choice.
If we increase the resolution or alphabet of the distribution N → ∞, then the entropy gets larger. If
we coarse grain by making C → ∞ then the entropy approaches zero and then ceases to be defined for
normal arithmetic rules [41]. The ambiguity lies in how we count the implicit dimension of the state space
(see figure 9 and the discussion below). The issue here is that a complicated process has more attributes
or ‘degrees of freedom’ than a simplistic view of the number line.
In thermodynamic statistical physics, the Boltzmann entropy mimics the Clausius entropy as a measure of the energy in a system, which by virtue of being distributed indistinguishably around the system,
has lost its capacity to do work. This is because statistical absorption is just as powerful as absorption by
a single state or node of a process. In either case, the ability to make distinctions over a prescribed scale
is the relevant issue. No measure of information can be conserved by processes that converges or diverges
into a change of degrees of freedom, on any scale, and hence cannot be restored without an injection new
boundary information for the inverse operation at or above the scale of aggregation.

4.3

Sources and sinks

A state is absorbing in a semantic graph because this reflects the interpretation we intend for it. In other
words, it’s no accident that we end up with absorption. The same is true in arithmetic if one is careful,
but there are cases where one is led to seek answers in ways that bump into problems concerning the
incompleteness of definition.
If we think if the values in the field as nodes in a graph, with arbitrary many nodes, then operations
that take us from one value to another may be represented as links with particular arrows that represent
the operational semantics.
25

Graph transformations are used widely in machine learning diffusion models for image reconstruction
and enhancement. They are also of interest here in a process knowledge representation for tracing the
contextual relevance of the map, which I’ll return to below.
Consider nodes 0,1,2 in the confluent junction in figure 7. The partial adjacency matrix is:


0 0 0
(32)
Â =  1 0 0 
1 0 0
From this, the transposed adjacency acts as a forward stepping operator over the vector landscape of internal node values, and the untransposed matrix is a backwards stepping operator when acting on internal
graph node values, represented as a vector ⃗v T = (v1 , v2 , . . .) :




0 f1 f2
0 0 0
ˆ
ˆˆB =  b1 0 0  ,
ˆF =  0 0 0 
(33)
0 0 0
b1 0 0
so that


0
F̂ ⃗v =  0
0

f1
0
0


 

f2
x1
(f1 x1 + f2 x2 )
.
0   x2  = 
0
0
x3
0

(34)

We note that B̂ and F̂ are not inverses of one another, since they both contain zero eigenvalues, i.e. have
determinants of zero. Operating on the graph’s internal state with this stepping operator, we see that the
values from nodes 1 and 2 are shunted onto node 0, with the weighting determined by the adjacency
matrix link weights. The original value at node 0 falls off the end of the absorbing node into the void and
is unrecoverable. If we now try to reverse this in order to restore the original information, we see that the
absorbing node wipes out the memory of the system rendering an inverse impossible without an input of
new information:

 


0
f1 x1 + f2 x2
0 0 0
 =  b1 (f1 x1 + f2 x2 )  .
0
(35)
B̂ F̂ ⃗v =  b1 0 0  
0
b2 0 0
b2 (f1 x1 + f2 x2 )
If we select the values for f⃗ and ⃗b appropriately, we can partially restore the original state, but not without
specific knowledge of the original configuration and the ability to inject appropriate values into other
nodes. The absorbing node x3 ’s value is lost forever unless we insert a value by hand (as a matter of
policy). and the initially unused source value x1 has been injected. The values of b would typically
involve division by the dimension or node degree ki n of the absorbing junction node 0.
(f1 x1 + f2 x2 ) → 1, b/f →

1
.
2

(36)

Why is the reverse operation not equal to the inverse matrix? This can be traced to the zero eigenvalues
(and zero columns in the F̂ operator). The computation of a direct inverse would require a division by
zero, which could yield any value, from eqn (29).
For a 3×3 matrix we can write the inverse explicitly for arbitrary real numbers a, b, c, d, e, f, g, h, i, j:


a b c
M = d e f 
(37)
h i j
26

The transposed cofactor matrix (adjugate) forms the the linear combinations which can be reverse engineered to yield cancellations or determinants, giving a formula:


(ej − if )
−(bj − ic) (bf − ec)
1
 −(dj − hf ) (aj − hc) (af − dc) 
M −1 =
(38)
det(M )
(di − he) −(ai − hb) (ae − db)
where det(M ) = a(ej − if ) − b(dj − hf ) + c(di − he). The difficulty arises in the scaling of the values
to renormalize the diagonal to ⃗1:
M M T = M T M = I = ⃗1.

(39)

For a sparse graph, most of the values a, b, . . . are zero. The ability to evaluate the expression is then
unclear, since the rules for rings and fields do not admit division by zero. Here there are so many zeroes
in multiplication that one has to see them as strings of operations rather than mere numbers.
The renormalization of the inverse is related to an overall factor of the determinant of the matrix. Since
the determinant is the product of the eigenvalues, it is zero if there if a zero eigenvalue, which corresponds
to a zero row or column. For a directed graph, this corresponds to a node which goes nowhere i.e. an
aborbing state.
The effect of B̂ F̂ is to eliminate the degree of freedom at x0 and to coarse grain the values of the others
so that they become equal. The only way to restore the original distribution is to encode the original values
by a memory operation M̂ (⃗x, F̂ ) → B̂. Indeed, this is schematically how diffusion models of machine
learning restore images: by training an inverse process to capture the process memory of destroying the
image through insertion of noise. The presence of 0−1 in the determinant or inverse indicates the need to
remember past history, a snapshot of the past to ‘roll back’ to, as pointed out in [38].
We can now associate the semantics of stepping and state exposure with the semantic spacetime
properties.
The same inverse notions apply to the semantic graphs. If we try to shift context up or down a branching graph, in the information hierarchy, crucial context may be lost. The relevance of the path is reduced
by the dimension of the possible alternative pathways. This loss is happening inhomogeneously all over
the graph where a process in ongoing. While this might initially seem harmless, making sense of the result
requires a continuous input of new information to keep the inverse on course–but, on course to where?
Deciding this selection requires an intentional act, i.e. an intentional insertion of policy information at
each stage. In diffusion models, this is provided by prompt information from a user.
As with logics, if one attempts to remember complete and precise information, then one would only
be able to generate results that were explicitly input. No recombinative mixing or ‘lateral thinking’ could
enter the process to create something new (however derivative).
In earlier work, I proposed a T-rank algorithm for maintaining a more stable entropy distribution
over a ⃗v by pumping graph emission self-referentially to counter absorption. While this works, it also
emphasizes that an ad hoc input is needed to obtain a stable answer, and that this ad hoc prescription
basically determines the outcome with possibly only a shadow of the original constraints intact.

4.4

Frobenious-Perron eigenvector theorem

The eigenvector equation
M⃗v = λ⃗v ,

(40)

for some matrix M and vector ⃗v has solutions called eigenvectors, which are intrinsic (eigen) properties
of the matrix in some sense. The equation can be applied to graph matrices by taking the adjacency matrix

27

M 7→ A, which is non-negative, over the nodes of a graph Γ(N, V ) 7→ (⃗v , A). This technique is widely
used in social network analysis to perform so-called importance ranking of nodes in the graph [37].
The Frobenious-Perron theorem for non-negative matrices states that the largest or principal eigenvector of any such graph will be entirely positive. More significantly, the semantics of this vector attach
to the eigenvalue equation: Iterating equation (40) represents a recursive propagation of node values over
links, weighted by the link values, which implies that multiplying any non-zero vector repeatedly by A
will converge ⃗v towards the principal eigenvector.
The purely positive addition of purely positive values must yield the highest eigenvalue. An undirected graph is in flow equilibrium, so there is no net direction to the movement of values over the links.
The equilibrium distribution of ⃗v in the principal eigenvector thus represents the reservoir water level at
each of the nodes (flow capacitance) at equilibrium. In social networks, this is equated with social capital
or ‘importance ranking’ (an important person is someone with many important friends).
For an undirected graph, where node connections propagate without opposition, all the value flows
along directed paths to sink nodes, where it pools. The normal algorithm for computing the eigenvector by
operating many times with A onto a vector of ones ⃗1 fails in this case to give the right answer, essentially
due to the ambiguity of division by zero. When there are zero rows (absorbing nodes) the effect of this
computation is simply zero. A more careful analysis shows that the vector components for absorbing
nodes should be non-zero as this is where all the flow piles up, but the multiplication by zero overwhelms
the normalization of the vector by λ. which may be zero itself for the absorbing nodes yielding 0/0 7→ 1.

5

Graph semantics of operational arithmetic computation

We can illustrate some of the semantic choices in basic arithmetic operations using graphs as the domain
‘space’ to model what we mean by the operations. The standard meanings are so ingrained in us from an
early age that it might seem strange to question them, yet doing so is quite instructive. The purpose of
doing so is not to necessarily change conventions or solve some inconsistencies, but to point out how a few
common themes trace back to interpretational ambiguities that we take for granted in mundane arithmetic.
These become crucial to our understanding when we adopt semantically rich knowledge representations,
such as in artificial reasoning.
Without adopting the standard jargon of rings and fields, arithmetic concerns the rules for counting
and measuring amounts of ‘stuff’. It uses formal quantities called x to model ‘amounts’ and handles
operations of augmenting and combining (+), depleting (-), duplicating (·) and sharing (/) operations.
These operations should work in both quantitative and qualitative interpretations. The abstractions either
expose or conceal information however.
In the algebra of rings and fields, the status of numbers as positions or as shifts is blurred into a single
set theoretic domain along the number line. Apart from this conventional interpretation, the question of
semantics remains ambiguous for graphical representations of operations. Should numbers be thought of
as value locations or as part of transformational operations? Should we treat numbers as events, things,
or concepts? And perhaps more pertinently, do we muddle these interpretations carelessly in dealing
with numbers? This question is particularly interesting in the service of Quantum Theory, where operator
algebras and numbers co-mingle extensively.
As an illustration of this, we can begin by looking at graph representations of basic arithmetic reasoning and ask what are natural interpretations for addition, subtraction, multiplication, and division where
the domain of mapping is no longer simply a single copy of the number line. Although this feels slightly
self-indulgent, it’s illustrative of the fundamental issues in attributing semantics to formal processes and
therefore underpins everything else in a form which is familiar to all readers. Let’s consider some examples, which might not be exhaustive.
The first question for semantics is to ask how we actually mean to represent a number to be added,

28

subtracted, multiplied or divided. There is no unique answer to this question, but we can naively imagine
people counting pebbles or abacus beads. The standard rules for calculating are guided by a principle of
closure around a set of numbers: when we combine numbers, the result should be a number of the same
‘type’.
+ : R × R 7→ R.

(41)

This is clear enough in the limited realm of mathematics, because the semantics of the ordered number
line R have already been defined (see figure 10). The number line is a key visualization of the process
that works well for a geometrical interpretation of addition, but less well for a bulk interpretation. In our
standard definition of division, for example, which is based on the primacy of the number line, there are
‘design issues’ to consider: if we wish our answer to a division operation to remain (mapped back into)
the scope of the number line, then we can no longer both map an agent of size x back to an agent of
size x and split the original inventory amount into a parts for all values without expanding the concept
of numbers to include fractional amounts. Once one introduces new numbers, negative numbers and so
forth, the consistency of the whole is jeopardized unless one can close the
√ operations convincingly. This
had resulted in the invention or discovery of symbols representing i = −1, and more recently ⊥ or Φ
for concepts including 1/0 [42, 43].
external reservoir

a

a

a

a

x

x
debt grain

(a+)

(b-)

L
L

L

/3
L

/3

L

L

L/3

/3

L/3
L/3

L
L

(c-i . )

(d / )

L
3L

L
L

(c-ii . )
(composite event)

Figure 8: Some semantic interpretations of addition, subtraction, multiplication, and division in graphical form. A
graph has more degrees of freedom than the number line automorphisms of rings and fields. Notice that multiplication
has at least two possible interpretations: as an outgoing amplification of nodes (with an expansive dimensional
meaning) or as an additive aggregation from nodes (mapping onto a single value more like the conventional ring/field
interpretation).

29

5.1

Arithemetic

Let’s consider the arithmetic operations on the number line as an example. These operations define
operations on rational numbers in graphical terms (see figure 8), but run into difficulties with irrational
numbers. In order to cover irrational numbers (which some mathematicians deny the existence of) one
has to introduce infinities and limits on the interior of agents. These are fascinating issues that can only
be mentioned in passing here (some discussions can be found in [43–48]).
Is there such a thing as a pure number? Arithmetic algebra defines unary 1 7→ −1 and binary 1+1 7→ 2
operators, in which the values are often pictured as locations on the real number line R1 . Euclidean
geometry builds on this idea to coordinatize Rn and make the explicit connection between value and
location in a spatial construct. A graph is an analogous structure to a Euclidean vector space
The algebraic structures of geometry and arithmetic are so ingrained in daily norms that we seldom
stop to confront the details of why these structures work. Representing these processes as graphs is an
interesting exercise in self-consistent representation, as algebraic graph treatments involve matrix arithmetic, which is a generalization of ordinary arithmetic for partially coherent parallel processes. The
stories or explanations we tell about these operations sometimes deviate from the actual rules and results provided by rings and fields. We can try to use graphical representations to elucidate the intended
meanings.

Figure 9: A compact representation of a zero multiplication interpreted graphically on a circular topology
with L rather than ∞ nodes. When inverting this, the number of possible shares is represented by the
number of arrows, like the vector results in eqns (53) and (54). The common interpretation of n/0 → ∞
could be associated with these arrows, so should the answer be n/0 → 1/L in a finite system?

5.2

Special arithmetic states 1 and 0

Key to establishing the axioms and theorems of rings, fields, and groups are the special values 1 and
0. These binary values have attained legendary status in the digital age, but their significances are more
important than binary arithmetic. They are the two stable fixed points of arithmetic:
1·1

=

n·1

= n

n·0

=

30

1
0

(42)

These are graphical processes (see figure 8) Here there is a special value 0 which is an absorbing state,
i.e. arrows that enter do not leave (figure 9)).
0×0

7→ 0

(43)

1×0

7→ 0

(44)

2×0

7→ 0

(45)

3×0

7→ 0

(46)

4×0

7→ 0

(47)

...L × 0

7→ 0.

(48)

In this interpretation, the proposal that x/0 7→ ∞ is a statement of the dimension or degeneracy of the
inverse map. The inverse map is not single values and thus a prescription for interpreting it is needed.
In machine learning diffusion models, for example, this inverse is interpreted as a Markov process and a
policy decision is used to invert the map to get ‘something from nothing’.
More generally one can introduce a new value (something like the square root of minus 1) such that
x/0 ≡⊥. But what kind of object is ⊥? In the finite graphical interpretation, one might imagine that the
dimension of the result would be L.

5.3

Addition as a graphical process

Translation is one dominant interpretation for additive arithmetic. Consider the expression:
x′ = x + a

(49)

where x, a are what we intend to mean as ‘numbers’ (counts or measures). This operation of addition has
two common interpretations that are easily distinguished in terms of agents. In order to use agents, we
only need to assume that the set of agents is countable. In this model A number may refer to:
1. A location of an agent on the number line R, which is a externalized total ordering of agents according to their number proper identities. The meaning of addition is then to translate or redefine the
labels from coordinate position x to position x+a. Relativity has implications for the interpretation
of these operations too.
2. The amount of interior holdings of the agent of some counter (e.g. money or energy etc). The
meaning of addition is then an augmentation of the holdings from x to x + a. We say this is interior
to the agent, because its position hasn’t changed in our new interpretation. However, one could
also argue that this is the same as introducing additional exterior dimensions that are private to
our invariant meaning for location (as one does in Kaluza-Klein or string theories of physics, for
instance). The question of where a new things come from, or how the semantics of a (an increment)
differ from the semantics of x (a state) is typically brushed aside.
The difference between interior and exterior ‘locations’, allowing us to distinguish exterior location from
interior holdings clearly has ‘boundary value’ implications.
Indulging the forbearance of the reader for a moment, let’s examine this in more detail, since the
issues are central to semantics of space and time and all graph based knowledge representations. We take
the first of these additive cases whimsically to mean a translation along the number line, as in figure 10:
In most cases, we think of x and a as being representative of numbers belonging to the same set, not
to different copies of a set that looks like R, yet this is misleading if not inaccurate. In both cases, the
closure of the rational or real numbers under addition is a formal identification of outcomes back onto the
same space, so while we write (41), we actually intend:
+ : R 7→ R × R 7→ R.
31

(50)

a

x

x+a

Figure 10: Addition pictured as a geometric translation. This is a common image in physics, where groups under
addition are used to represent motion.

The final map involves a loss of information. If we introduce an entropy of representation For other
operations one gets away with the same final mapping.
From a geometrical perspective we can think of x as a position, and a as being a argument to an
operation that shifts the state of our marker from x to x + a. Implicit in this idea is, of course, the
idea that the successive values along R1 are totally ordered so that the coordinatization is in one-to-one
correspondence with elements of some spacetime. If we pedantically represent (49) as an operation on x,
then we could write:
Ôa x 7→ x′ : |x′ | = |x + a|,

(51)

i.e. an operation of type argument or magnitude a on the initial position x x leads to a new location, in
which the location x′ has a label whose value corresponds to x + a. Without clear semantics, this is only
a tautology. Indeed, it is only convention that would lead us to believe the result of operating on x with
Ôa would be a position and not some new thing. This is not clear from (41).
Next, consider the interpretation as an interior amount. Aggregation is one kind of addition. We might
call it ‘semantic addition’ as opposed to addition as a translation along the number line. Now location and
translation are exchanged for inventory count and input amount, which combine back to a new inventory
amount. The question of where the input comes from (a different place than the agent’s own storage,
thus violating conservation) is sidestepped in the same way one sidesteps the semantics of changes in
thermodynamics: by inventing fictitious infinite reservoirs that are approximately conserved and so on.
This is a familiar trick in handling arithmetical anomalous cases too, such as with division.
To further illustrate the points here, consider an alternative matrix representation of addition, which
acts as a bridge between the interior and exterior forms, since it uses an explicit extra dimension to
distinguish and represent the add and a (see also [38]).

 




1
1 0
1
1
=
7→
(52)
x′
a 1
x
x+a
In this form, x and a belong to different ‘Euclidean’ dimensions, or rows in the row space of the matrix.
Clearly the expression also applies even when a = x, so 2x = x + x, though the name 2x now assumes
a new aspect of the algebra in linear multiplication. An entire chain of interpretation is implicit in these
semantics. In (52), the original meaning is embedded in the new, so this is hardly progress. However, it
serves to illustrate that we can make representations as complicated as we like, with as many dimensions
as we see fit. Matrices are of interest in this case, because they offer the bridge between graphs and
Euclidean vector spaces. We have intuitions about both, but these are easily muddled.
32

In a higher dimensional vector space Rn or a graph Γ(N, L), we need to select a direction from the
possible independent directions available. Directional generators in Rn can be made easily from suitable
matrices or tensors in a given coordinate system, assuming homogeneous and isotropic spaces. For a
graph it is generally more complicated, because there is no consistent set of directions from which to
choose: each point points to a select set of neighbour destinations, which is typically different at every
node. At the same time, the entire graph adjacency matrix can be thought of as a stepping operator for
propagating interior field values from node to node [37].
In a graphical representation, the superficial similarities between Γ(N, L) and Rn are clear, and are
frequently exploited in embeddings, e.g. for artificial intelligence feature representations for artificial
neural networks, in which one plays with interior and exterior dimensional representations freely to enable
independent processes.

5.4

Subtraction as a graphical process

Subtraction semantics x − a imply taking away part of a measurable state. Once again, our interpretation
will depend on the how we give meaning to x itself. As a translation, subtraction is the same as addition
with only a reversal of direction (assuming that the reverse direction exists, which is assumed in vector
spaces but is not generally true in graphs or vector fields). This is a convenient group theoretic property,
because we need an inverse for every operation to complete a symmetry.
If, on the other hand, x refers to the amount of inventory held by an agent there is no opposite
direction for filling states, unless one take on the concept of debt. This occurs in physics too because
of the attachment to a principle of conservation. Whether the conservation principles are actual physical
truths about the universe or simply self-consistent ways of counting slowly varying bulk quantities is
probably still unresolved. The changes in the way money is allocated point to a similar issue. Money is
not conserved but we calculate with it as if it were. This is the purpose of rings and fields: to uphold a
simple set of semantics that broadly enforce conservation of amounts.
The addition and subtraction thus have similar semantics but with opposite effect on the final states of
an agent. A translation forwards is similar to a translation backwards. An addition of inventory is similar
to a removal of inventory, which the same question about what become of the balance of the change.
Apart from the change of flow direction for a, subtraction is not substantially different from addition until
we reach amounts that involve subtracting from zero. This leads back to the question of conservation and
into what ‘afterlife’ these amounts a actually go to. The concept of ‘debt’ is based on this conundrum,
and the resolution involves playing with coarse grains of time over which balance is restored and ordering
can be sacrificed for the greater good of maintaining the conservation mythology. Subtraction then forces
us to embrace a wider world of dynamical behaviours, detailed balances, and on going interactions with
external reservoirs in order to give these ideas meaning. In doing so, it forces and explicit introduction of
averaging over time or space, repackaged by sleight of hand into coarse grained or statistical conservation.
If we return to the translation interpretation of addition, pushing beyond the end of the number line
would mean losing the value altogether. Topology offers alternatives here. Either translations fall off
the end of a finite number line into a void, or one might identify the ends into a toroidal or spherical
topology (a circle in one dimension), in which case a translation enters into the realm of modulo (‘clock’)
arithmetic, wrapping around with remainders. All calendar phenomena follow this approach, with finite
repetitions. Multi-valuedness of counting functions on the finite space is avoided by introducing more and
more dimensions to the clock (minutes,hours,days,years, etc) for the parameters to spread into to answer
the question of where the extra information goes. This is equivalent to adding new nodes to a graph to
prevent paths from coming to an end.
Finally, if one is unable to map all values back into a closed compact set of dimensions as in a
ring, then one may attribute the growth to information loss by introducing a coarse-graining concept for
garbage collection to wipe away unrecoverable information as an undesirable. Again, thermodynamics

33

(which is an explicit agent model of energy phenomena) has confronted the quantitative aspects of these
issues before. Our goal here is to extend this to the semantic aspects too.
The semantics of conservation quickly point out an unavoidable link between statistical (aggregate)
information and elementary changes, which leads to ideas of entropy.
In both cases, the algebra of rings gives a convenient answer to a question, but by riding roughshod
over specific semantics and normalizing a response. The answer provided however is not necessarily
an an answer to the question we intended to ask. Theoreticians learn to be cautious in the use of these
conventions.

5.5

Multiplication as a graphical process

Multiplication addresses the duplication, triplication, and n-ification of an amount. However, it also plays
a role in the meaning of repeated addition. One sees both interpretations in play in mathematics: multiple
addition has ‘translational’ semantics, while multiplication of directions has Cartesian ‘direct product’
semantics. Again, our understanding stems from integer amounts, and generalizes conceptually to real
numbers later. Once defined for integers, ring and field algebras allow one to close multiplication by
non-integer values. (figure 8)
Once again there are interior and exterior interpretations of multiplication as an operation. What
aspect of x do we wish to multiply? Referring to figure 8c, we could start with an agent whose inventory
is x and wish to multiply the inventory amount within the agent, or we could imagine a multiplication
of the agent itself, as a container, containing the equal amounts of inventory. No now x is an agent with
certain holdings, and a is an operational parameter with no direct connection to the number line. It is not
a distance. It’s semantics are new.
As before, with addition, we conventionally choose that x and a and x · a all map to the same space
R, and so (in agent terms) we want to map each starting agent to a copies that are of the same type, and
importantly contain the same amount of inventory. This only works in practice for integer multiples, but
we can then postulate the concept of partial agents to complete the calculational picture, in the same way
that we use debt to delay the realization of the amount in practice.
Once again, these issues are harder to avoid in physics, and have been confronted in thermodynamics and statistical physics by Boltzmann and others. There are deep connections here to the notion of
distinguishable and indistinguishable states and ‘particles’.
In ring algebra, we can think of multiplication as as successive addition. This is a result of the final
automorphism, mapping outcomes back onto a the original number line. While an external observer can
attest to the consistency of the answer in a counting scheme, several additions are very different from the
commonplace everyday semantics of multiplication (which involves manufacturing new copies of a state)
to make more things.
Conveniently, the association of inverse multiplication with division helps to cement this. A similar
association occurs in the Fundamental Theorem of Calculus, which proves that indefinite (symbolic)
integration is the inverse of differentiation and vice versa. The semantics of differentiation as a gradient,
and integration as a summation do not obviously make this clear, yet there is a methodological precision
(involving the infinitesimal limits of intervals that allows the operations to be co-related.
What single aspect of the operation should be mapped back into R: is it the amount received by each
agent, the number of copies, or the total inventory of all copies?

5.6

Division

Last but certainly not least, the semantics of division drive us more forcibly into the realm of agent
interpretations (figure 8). Division has no obvious translational interpretation, but it can be related to the
scaling and partitioning of distances—dividing a journey into a number of ‘legs’, or a procedure into a

34

number of subroutines. This interpretation is connected to the renormalization group in mathematical
physics [49].
A more common conception for division is simply to share a bulk amount between a number of
recipients, e.g. in a marketplace. In agent terms, there is (or at least could be) a container or binding force
to hold the multiple separable parts in different ‘buckets’. In the absence of further information, a sharing
out of x into a parts implies that the amounts are a-furcated into equal measures. There is no particular
reason why the measures should be equal, except for a sense of simplicity and symmetry. Prime numbers
become an immediate source of worry: what if the inventory is already atomic or indivisible, how can
we share amounts that do not divide exactly? Already we are faced with the invention of real number
semantics or the concept of remainders. What does it mean to share out the holdings of an agent into
equal parts of the same size, with or without remainders? A remainder turns a division not into a single
number, but a doublet of (share per recipient,remainder):
5/3 7→ (1, 2)

(53)

Matters of definition play an underestimatable role. Division, like multiplication, does not yield a single
unambiguous interpretation for its answer. Are we counting the number of partitions, the total amount of
stuff shared out, or the size of an individual share? The answer could, after all, refer to either the number
of agents with equal shares or the amount within each agent.
The usual answer gives a value which is the average size of the original amount given to each of the
agents sharing it. Since this is equal, by argument, there is a unique value that can be mapped back into
the original system of numbers. It’s meaning is lost, however. If we share a proton between three agents,
they would all get different quarks with unequal properties. Only by throwing away information can the
result be a single value rather than a vector of the partitioning.
=x
x
x
x

1/3
1/3
1/3

=x
x
x

(3x)

x

1/3
1/3
1/3

(3x)
(3x)

x
x

x
x

x
x

x

Figure 11: Multiplication and division as inverse processes. Here we represent 3x/3 = x as a process graph.

Division by N is a partitioning of the a set into N parts. This makes obvious sense for all numbers
except for numbers less than 1. For example, division by 2 leads to two sets that are defined to be
equivalent and thus the return value is the result for one of them. We do not write:
4/2 ̸= (2, 2)
35

(54)

This could be interpreted in two ways: the 2 could mean each received a share of 2 or that there are two
copies (a ‘redundancy’ or ‘degeneracy’ of 2). What about values smaller than 1? Dividing a set into half
agents would mean that there are
x
(55)
1
2

implies that half agents would be twice as many
x
=?
(56)
0.5
The conventional answer of 2x here involves a renormalization of amounts. It is not so much an answer as
a redefinition of the scale used for the answer. Again the semantic legerdemain leads to the convenience
of an inverse operation to multiplication, at the expense of metaphysical caution.

5.7

Division by zero semantics

The more fraught issue of division by zero must also be handled by a more careful convention. It’s
possible interpretations are intimately caught up in the processes it might represent—which should issue
a warning to those who wish to fully abstract a universal meaning for numbers. Dividing a set into
zero parts does not have an obvious mundane meaning. It could mean that the result is nothing as noone receives any share of the initial state, or it could mean everything since the initial amount remains
undivided. Alternatively, one could think of the limit 1/0 → ∞ as the metaphysical effort or force
needed to break through the barrier of zero recipients, punching a hole through to the number line. That’s
an interesting picture but with semantics that apply only to certain process scenarios.
In another sense, multiplication and division share many similarities, not only as adopted inverses for
fields. Both involve nodes that can be considered to absorb or redistribute values, with arrows responsible
for moving, aggregating, combining, distributing, and even creating and deleting information based on
the topology and the semantics of the graph.
Conservation of some counter (flow continuity) is one of the most common criteria for defining processes, in order to have a consistent normalization of quantities. This is pervasive in physics at least for
idealized closed systems. Suppose we argue graphically (figure 8) that division is the sharing of an incoming amount like a flow into a parts; then why do we not follow multiplication and ask for the answer to
represent the sum inventory of all the participants? Then one would simply say that x/a = x because the
amount of information hasn’t actually changed: the total amount was conserved. Alternatively, we could
introduce a partioning into new dimensions, as in modulo arithmetic, and say that x/a 7→ {b1 , b2 , . . . ba }
for some bi ∼ x/a. Then we sacrifice the property of mapping back to an object of the same or similar
type, as an inverse for multiplication within R.
In mapping the result back into R, we make a choice about which kind of answer we want to keep.
Choosing division to be the inverse of multiplication has many advantages and few anomalies, but the
anomalies point to cases where the result makes more since in other interpretations. Division by zero is
the obvious case. As with the debt concept for subtraction (e.g. subtracting a negative amount becoming
addition of a positive amount), division by zero involves handing information across the boundary from
some ‘interior inventory’ to or from ‘somewhere else’. One cannot actually realize a negative amount in
order to subtract it, but we can post hoc interpret an addition as having the same effect. For equilibrium
processes, however, one can take away a depletion process leading to a net shift in the balance to a positive
movement. This is like the concept of ‘electron holes’ in solid state physics. In the case of division by
zero, however, the handover to zero recipients is manifestly cavalier as it throws away the meaning of a
transfer altogether. There is no recipient. In Promise Theory, however, this does correspond to an physical
scenario, where there is an an agent offering x but there being no agent accepting the amount:
+x

x · (0) : x −−→ {}
36

(57)

If an agent insists on propagating (imposing) the value, there is no agent to catch the state, we have to
define what happens to it. The question of where the information goes is harder to paper over with a
simple concept like a minus sign. There is now a translation into the void, from which no inverse can
recover the value. Multiplication involves copying so it does not conserve amounts, unless we postulate
its inverse.
If we define the answer to division by the average share amount received by each agent, then a conventional renormalization of scale would send the amount to infinity as a limiting process, but only because
we want to imagine integer divisors as discussed above. If we ask instead what is the total inventory for
all agents, thinking again about conservation, we multiply the shares by the recipients:
x/0 7→ b · 0 = 0.

(58)

Then we admit that the shares have been operated away by shrinking the size of the receiver set to
nothing, violating the conservation. Clearly the absorbing fixed point property of zero is problematic not
so much semantically as operationally. Once again the information is simply cast into the void with these
semantics. Which of the zeroes is supposed to win? The existence of an agent as a container surely wins
over its imagined contents. If we have no semantic attachment there is no rational way to resolve the
difference between the zero. This might not matter in the context of a neutral calculation, but it might be
the different between an image of a dog or a cat in a process of image reconstruction [18].
What then is an appropriate return value for the function associated with this division? We have
several returnable characteristics for the process: the input values, the dimensions of the graph, values
associated with the ring etc. What if the return value is the number of agents involved in the sharing? We
can imagine the sharing process as being a fair weighted split of the input x into L pieces.
x
dim({L})

(59)

x
dim({x} 7→ 0)

(60)

x/L 7→

x/0 7→

When we interpret with agents like this, we are assuming that they are either blank “stem cell” agents
initially or that they can track where stuff comes from. Semantic labels make the latter possible in
computing and biology, but elementary agents may not have enough resources to track this information.
If we think of the division as part of a process involving agents with memory of state, then the recipient
agents might already contain a non-zero amount to begin with (like a bank account balance), so that their
final state becomes the answer rather than simply the dividing transaction. If the agent already had a
non-zero offset b0 which could regularize the value b = b0 + x/dim({x}) it alters the way we handle the
outcome. For R dim(x 7→ 0) 7→ ∞, but for a ring of dimension L, dim(x 7→ 0) = 1/L, which would
imply x/0 7→ xL < ∞.
Sharing nothing between no recipients is no more or less well defined. The only natural answer within
the scope of the real numbers is 0. The only value for which this makes invariant sense is x = 1, because
the absorbing property of 0 interacts with the idempotence of 1 as a stable fixed point under multiplication
except for zero. However, in this configuration, the division by zero eliminates the zero issue.
k

x
x
1
= ′ = k =7→ 0
0
k0
0

(61)

This in turn suggests that, if we wish to map 0/0 back into R then the only plausible value is
0
x
=0· =0
0
0
37

(62)

The problem is not so much the value as the process semantics of the operation (see figure11).
Finally, we may note briefly that there is another example of division of states in which a zero state
can be inverted unambiguously into a finite integer number. This is for ladder operators in Fock space
algebra [50] or Lie algebra root and weights [51]. The zero state is defined by an absorbing state for the
annihilation operator â.
â|1⟩ =

|0⟩

â|0⟩ =

0.

(63)

Reconstruction, like the F̂ and B̂ graph operators for directed processes, give:
(a† )n |0⟩ 7→ |n⟩

(64)

0/0 = 1, 0/02 = 2, . . . 0/0n = n

(65)

It suggests that one could define

This is because the logarithmic property transmutes powers into addition, introducing a new question
associated with the definition of logarithms over rings and fields. Clearly the full semantics of divisions
are poorly represented by rings and fields, in a similar way that analyticity is incompletely by the reals
without a wider scope of complex numbers.

6

Conclusions

Building on the Semantic Spacetime model as a set of guiding principles for graph representations, we
can simplify the selection of proper link identification by adopting the γ(3, 4) representation to remove
unnecessary ambiguities, without adopting ontology or a detailed first order logic. The graphical properties of the algebra are then postulated to be compatible with all expected processes and inferences can
now be written down explicitly and be verified by matrix algebra.
It remains for future work to understand how to understand whether knowledge is trustworthy [52,53]
by the balance between the different node etcs e, t, c affect the reliabilty of a knowledge representation.
If a knowledge structure contains only concepts, it lacks grounding in truth and can easily disconnect
with reality. Could this explain what we are seeing with artificial intelligence knowledge, fake news, cult
beliefs and extremism? This is a problem of interest for a more empirical review.
Technically, the presence of absorbing states is inevitable in graphs with non trivial adjacencies. These
absorbing states need to be interpreted carefully. If we consider the graphs to be dynamic flows, they lead
to loss of information at the edges of the system. The wider problems of consistent transformations within
a finite structure have a common theme: the absence of invertability, due to states that are absorbing. In
Markov processes, there is no memory to prevent loss of information, but one assumes that information
is redistributed in such a way that it isn’t actually lost. Absorbing states behave in a similar way, until the
moment we want to reverse flows that lead to them. Remedies are analogous connection with division by
zero remedies. It would be interesting to study the effects of the various remedies on matrix inversion for
directed graphs in more detail.
Axioms and logical primitives already stand alone as ‘boundary information’ or irreducible knowledge. Such boundary information is thus intimately related to the absorption and emissions of nodes in a
directed graph. As one uses the SST structures to scale structures, such features will continue to exist on
many scales. Whole regions of a graph can be absorbing. Hierarchical graphs, such as taxonomies and
spanning trees, also have nodes that are both the beginning and end of some flow. Finiteness demands
this. Only in quasi-continuum models, groups and semi-groups, are we able to define ‘translationally
invariant’ systems that go on and on in different directions. So the infinity of possible outcomes for end
38

states avoids dealing with beginning and end states. This absence of boundary is used frequently in physical models such as field theories, to argue for smooth continuity. Clearly, the concept of zero and infinity
are complementary in this process sense.
There are no moving bodies in the Semantic Spacetime discussed here, however intrinsic properties
of space can be passed along to relocate from node to node or from agent to agent. This is called Motion
Of The Third Kind [1, 54]. In a sense it is forced to ‘invent’ the semantic split between matter and
spacetime in order to resolve ambiguities about changing states, as natural philosophers found necessary
in the formative years of physics. The semantics of material and conceptual constructs are similar. It’s
also reminiscent of the arguments about whether one should call interior properties real or not, which
continues to rage in the field of Quantum Mechanics.
In modern Artificial Intelligence or reasoning models, where many of the techniques originate from
the palette of mathematical physics, there is a tendency to paint path selection and inference with the
broad brush of probability theory, without questioning too much what the probabilities mean. The labels
that can remember inverses have to be trained explicitly at some expense in shadow representations of
knowledge. The relatively recent forays into context and relevance modelling are suprisingly overdue in
artificial neural network representaions. Semantic Spacetime belongs to this effort. Semantic spacetime is
not a natural language model. Natural language remains a very different way of compressing intentional
descriptions with rich semantic content, which undoubtedly relies on the capabilities of an evolved brain
for bridging the gulfs between broad and sometimes audacious inferences. The possibility of scanning
natural language and compiling a compact SST representation is nevertheless an intriguing possibility
of great interest in connection with generative Artificial Intelligence, with or without Large Language
Models.
A software implementation of this work is available at [55].

References
[1] M. Burgess. Spacetimes with semantics (i). arXiv:1411.5563, 2014.
[2] M. Burgess. Spacetimes with semantics (ii). arXiv.org:1505.01716, 2015.
[3] M. Burgess. Spacetimes with semantics (iii). arXiv:1608.02193, 2016.
[4] M. Burgess. A spacetime approach to generalized cognitive reasoning in multi-scale learning.
arXiv:1702.04638, 2017.
[5] J.A. Bergstra and M. Burgess. Promise Theory: Principles and Applications (second edition).
χtAxis Press, 2014,2019.
[6] W3C.
Defining n-ary relations on the semantic web:
http://www.w3.org/TR/2004/WD-swbp-n-aryRelations-20040721/.

use

with

individuals.

[7] Mark Burgess. Testing the quantitative spacetime hypothesis using artificial narrative comprehension (i) : Bootstrapping meaning from episodic narrative viewed as a feature landscape, 2020.
[8] Mark Burgess. Testing the quantitative spacetime hypothesis using artificial narrative comprehension (ii) : Establishing the geometry of invariant concepts, themes, and namespaces, 2020.
[9] D. Watt. Programming language syntax and semantics. Prentice Hall, New York, 1991.
[10] B.C. Pierce. Basic Category Theory for Computer Scientists. MIT Press, 1991.

39

[11] B. Coecke adn A. Kissinger. Picturing Quantum Processes. Cambridge, 2017.
[12] R.W. Langacker. Cognitive Grammar, A Basic Introduction. Oxford, Oxford, 2008.
[13] I. Heim and A. Kratzer. Semantics in Generative Grammar. Blackwell, 1998.
[14] J. Strassner. Handbook of Network and System Administration, chapter Knowledge Engineering
Using Ontologies. Elsevier Handbook, 2007.
[15] G. Deutsche. The Unfolding of Language. Academic Press, 2005.
[16] M.B. Hesse. Forces and Fields: The Concept of Action at a Distance in the History of Physics.
Dover, 1962.
[17] G.R. Grimmett and D.R. Stirzaker. Probability and random processes (3rd edition). Oxford scientific publications, Oxford, 2001.
[18] C.M. Bishop and H. Bishop. Deep Learning. Springer, 2023.
[19] N.M. Cirone, A. Orvieto, B. Walker, C. Salvi, and T. Lyons. Theoretical foundations of deep
selective state-space models, 2025.
[20] J. A. Bergstra. Division by zero, a survey of options. Transmathematica, pages 1–20, 2019.
[21] M. Burgess. Smart Spacetime. χtAxis Press, 2019.
[22] R.P. Feynman. Space-time approach to quantum electrodynamics. Physical Review, 76:769, 1949.
[23] F.J. Dyson. The radiation theories of tomonaga, schwinger and feynman. Physical Review, 75:486,
1949.
[24] L.F. Abbott. Introduction to the background field method. Acta Physica Polonica, B13:33, 1992.
[25] F. Reif. Fundamentals of statistical mechanics. McGraw-Hill, Singapore, 1965.
[26] J. Myrheim. Statistical geometry. CERN preprint TH.2538, August 1978.
[27] R.D. Sorkin. Causal sers: Discrete gravity. arXiv:gr-qc/0309009, 2003.
[28] S. Surya. The causal set approach to quantum gravity. arXiv:1903.11544 [gr-qc], 2019.
[29] E.L. Robertson. Triadic relations: An algebra for the semantic web. Lecture Notes in Computer
Science, 3372:91–108, 2005.
[30] N. Fanizzi, C. d’Amato, and F. Esposito. Evolutionary clustering in description logics: Controlling
concept formation and drift in ontologies. pages 808–821, 09 2008.
[31] Michael Uschold. Ontology and database schema: What’s the difference?
10:243–258, 2015.

Applied Ontology,

[32] R. Milner. The space and motion of communicating agents. Cambridge, 2009.
[33] Mark Burgess. In Search of Certainty - The Science of Our Information Infrastructure. χtAxis
Press, November 2013.
[34] A. Couch and M. Burgess. Compass and direction in topic maps. (Oslo University College preprint),
2009.
40

[35] A. Couch and M. Burgess. Human-understandable inference of causal relationships. In Proc. 1st International Workshop on Knowledge Management for Future Services and Networks, Osaka, Japan,
2010.
[36] Jon M. Kleinberg. Authoritative sources in a hyperlinked environment. Journal of the ACM,
46(5):604–632, 1999.
[37] J. Bjelland, M. Burgess, G. Canright, and K. Engø-Monsen. Eigenvectors of directed graphs and
importance scores: dominance, t-rank, and sink remedies. Data Mining and Knowledge Discovery,
20(1):98–151, 2010.
[38] M. Burgess and A. Couch. On system rollback and totalized fields: An algebraic approach to system
change. J. Log. Algebr. Program., 80(8):427–443, 2011.
[39] C.E. Shannon and W. Weaver. The Mathematical Theory of Communication. University of Illinois
Press, Urbana, 1949.
[40] T.M. Cover and J.A. Thomas. Elements of Information Theory. (J.Wiley & Sons., New York), 1991.
[41] Jan A Bergstra and John V Tucker. Expressing entropy and cross-entropy in expansions of common
meadows, 2025.
[42] Jan A. Bergstra and Alban Ponse. Division by zero in common meadows, 2021.
[43] J.A.D.W. Anderson. Perspex machine ix: Transreal analysis. In Electronic Imaging. San Jos,́ 2007.
[44] J.A.D.W. Anderson, N. Völker, and A. A. Adams. Perspex machine viii, axioms of transreal arithmetic. In Proc. SPIE 6499. Vision Geometry XV, 2007.
[45] J.A. Bergstra and J.V. Tucker. The rational numbers as an abstract datatype. Journal of the ACM,
54:1–25, 2007.
[46] J. A. Bergstra and J. V. Tucker. The data type variety of stack algebras. Annals of Pure and Applied
Logic, 73(1):11–36, 1995.
[47] J. A. Bergstra and J. V. Tucker. Division safe calculation in totalised fields. Theory of Computing
Systems, 43:410–424, 2008.
[48] J. A. Bergstra, Y. Hirshfield, and J. V. Tucker. Meadows and the equational specification of division.
Theoretical Computer Science, 410:1261–1271, 2009.
[49] G.I. Barenblatt. Scaling, self-similarity, and intermediate asymptotics. Cambridge, 1996.
[50] S. Schweber. Relativitsic Quantum Field Theory. Harper & Row, 1961.
[51] H.F. Jones. Groups, Representations, and Physics. Institute of Physics, 1998.
[52] M. Burgess. Notes on trust as a causal basis for social science. SSRN Archive, available at
http://dx.doi.org/10.2139/ssrn.4252501 (DOI: 10.2139/ssrn.4252501), August 2022.
[53] M. Burgess. Trust and trustability: An idealized operational theory of economic attentiveness.
preprint paper (DOI: 10.13140/RG.2.2.26862.28480/1), April 2023.
[54] M. Burgess. Motion of the third kind (i) notes on the causal structure of virtual processes for
privileged observers. DOI: 10.13140/RG.2.2.30483.35361 (notes available on Researchgate), 2021.
[55] M. Burgess. Sstorytime project. https://github.com/markburgess/SSTorytime/
tree/main, 2025.
41

A

Examples

Only events can lead to events.
1. Hammering leads to noise
Possible: “The activity of hammering” L(leads to) “the event of noise” Wrong: “The activity of
hammering” C(contains) “noise”
Possible: “The concept of hammering” E(may have property) “noise”
2. His stubbornness led to indignation
Possible: “The event in which he was stubborn” (led to) “An event in which there was indignation”
Wrong: “The concept of stubbornness” E(has the property) “the concept of indignation”
3. That cake is just like your house!
Unlikely: “The cake (thing)” N(is similar to) “your house (thing)”
Likely: “The appearance of the cake (concept)” E(has property of mapping to) “the appearance of
your house”
4. The virus caused his death
Impossible: “The virus concept or thing” L(led to) “the event of his death”
Possible: “the viral infection event” L(led to) “the event of his death”
5. Professor Plumb murders Ms Scarlet in the library
“The event of plumb murders scarlet” (is an example of) “concept of murder”
“The event of plumb murders scarlet” E(has the attribute) “concept of murder”
“The concept of plumb murders scarlet” E(is an example of) “concept of murder”

42

