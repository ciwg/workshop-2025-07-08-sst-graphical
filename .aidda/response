File: /home/stevegt/lab/cswg/workshop-2025-07-08-sst-graphical/Semantics_Graphical.txt
```
Agent Semantics, Semantic Spacetime, and
Graphical Reasoning
Mark Burgess

arXiv:2506.07756v2 [cs.AI] 13 Jun 2025

June 16, 2025
Abstract
Some formal aspects of the Semantic Spacetime graph model are presented, with reference to its
use for directed knowledge representations and capacity for process modelling. A finite γ(3, 4)
representation is defined to form a closed scaffolding set of operations that can scale to any degree of
semantic complexity, with a highly simple rule set and a universal ontology. The Semantic Spacetime
postulates bring predictability when reasoning, with minimal constraints on following pathways
in graphs. The ubiquitous appearance of absorbing states in any partial graph means that certain graph
processes leak information and represent entropy changing processes. The issue is closely associated
with the issue of division by zero, which signals a loss of closure and the need for manual injection of
remedial information. The Semantic Spacetime model (and its Promise Theory) origins help to clarify
how such absorbing states are associated with boundary information where intentionality can enter.

Contents
1

Introduction
1.1 Spacetime characteristics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1.2 Euclidean vs graph . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1.3 Symmetries . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

2

Semantics of spacetime and spacetime semantics

We define the Semantic Spacetime model first in terms of the kinds of links that
connect nodes, and then in terms of the kinds of nodes that make sense with those
connections.
In a process view of the world, each point is a process event that needs to be
justified rather than postulated as a generalized container for motion. This view is behind
the famous relationship between Feynman diagrams and the algebraic and differential
generating functional representations of Schwinger

and Tomonaga in Quantum Field Theory [22–24]. Further, both in Quantum Field Theory and in Group
Theory one has ladder stepping operators (creation and annihilation) generating graphs by the action of an
algebra of operators over abstract states. Underpinning all those variants are a mathematical description of
rings and fields that underpin everything else practically as axioms. Questioning these feels unproductive,
yet graph representations in particular have interpretations of arithmetic that poke holes in the rules we
take for granted. One has to be clear about whether returning to the same state is actually the same node
in a graph or a new version of a corresponding node which is similar but distinguishable. These notions
have been long understood in statistical mechanics and thermodynamics [25]. In this regard, a semantic
spacetime is akin to causal sets [26–28].

2.1

The purpose of Semantic Spacetime

Semantic Spacetime can be used in a number of areas including agent modelling, collaboration networks
like supply chains, service mapping, formal reasoning, and narrative representations. The role of a knowledge
representation is to capture experiences and to abstract and generalize them, associate and collect
them into meaningful buckets that makes the knowledge easy to access and interpret. For example, the
flow of processes through a general graph can represent:
• Scene description and forensic reconstruction.
• Flow of utilities (e.g. electrical, water) through grids etc.
• Communications networks like the telephone and Internet networks.
• Dependency networks, like supply chains, ownership and market structures.
Any amount of detail can be added to a graph of meaning, in principle. Recent developments in
Large Language Models has placed much emphasis on generating fluent language, which is a vast area
of subtle meanings. Before we get as far as semiotics and linguistic considerations, we need to look into
the constraints on meaning imposed by topology and conservation of information. Dynamical considerations
will always trump semantic considerations, because flow is a dynamical phenomenon semantics
piggyback on quantitative amounts.

2.2

The 4 relationship types

The Semantic Spacetime model is basically a graph structure in which nodes and links are used to represent and expose the compressed meaning normally described in more fluent natural language. This is in
the spirit of formal languages and state machines; however, it operates on a coarse level from the assumption that anything that happens is a process than can be expressed in terms of some kind of spacetime
journey.
The four types arise from the model of agents. In an agent model of causality, which is fully localized, processes and information only happen inside agents unless the agents cooperate to represent larger
structures. However, it is clear that activity only happens by agent states changing. Ideas and structure
cannot be imposed upon them from outside without violating their local autonomy. This is a strong form
of locality as expressed in physics. Based on the semantics of agents, and the model of intent developed
by Promise Theory, the semantic spacetime model settles on four basic arrows between nodes in a graph
that are postulated to be sufficient for any semantic description [1–3]. This remains a hypothesis for now,
but it is not a particularly original one. Various authors have suggested that spacetime concepts underpin natural language, on the basis that they are the only objective concepts an organism has to bootstrap
meaning from. Promise Theoretically, one derives four kinds of promise or relation between nodes in
order to represent spacetime processes. These are called (see table 1):
6

ST T YPE
ST 0 = ‘N ’
‘NEAR’

ST 1 = ‘L’
‘LEADS TO’

ST 2 = ‘C’
‘CONTAINS’
ST 3 = ‘E’
‘EXPRESS
PROPERTY’

F ORWARD
is close to
is similar to
sounds like
is correlated with
enables
causes
precedes
to the left of
contains
surrounds
generalizes
has name or value
has property
expresses attribute
promises
has approximation

R EVERSE
is close to
is similar to
sounds like to
is correlated with
depends on
is caused by
follows
to the right of
is a part of / occupies
inside
is an aspect of / exemplifies
is the value of property
is a property of
is an attribute expressed by

S PACETIME STRUCTURE
PROXIMITY
“near”
Semantic symmetrization
similarity
ordering
GRADIENT/DIRECTION
“follows”
boundary perimeter
AGGREGATE / MEMBERSHIP
“contains” / coarse graining
qualitative attribute
DISTINGUISHABILITY
“expresses”
Asymmetrizer

approximates

Table 1: Examples of the four irreducible association types, characterized by their spacetime
origins, from [3]. In
a graph representation, ‘has attribute’ and ‘contains’ are clearly not independent, so implementation details can still
compress the number of types.

• 0 NEAR: a directionless assertion of equivalence or proximity between two nodes
• ±1 LEADS TO: a temporally or causally ordered arrow denoting a sequence of events
• ±2 CONTAINS: a spatially ordered collection of containment regions
• ±3 EXPRESSES: a locally promised attribute of a node or distinguishing mark
The directionless type 0 (proximity) may be interpreted as an equivalence, approximately equal to, close
to, etc. Type 1 (linear sequential order) may represent time, or unidirectional ordering, causation, dependency, etc. Type 2 (containment) may represent membership in a group, generalization of a collection of
concepts, location inside or outside a perimeter, etc.
See table 1, which originally appeared in earlier papers [3, 4] with some errors. An alternative way
of listing the 4 types is in a tabular form (see table 2). Some of the relations address physical material
attributes, while others represent virtual or conceptual attributes used to describe scenarios.
• Physical is also assumed to refer to properties that are external to the agent, and are therefore
‘tangible*, ‘material’, or even ‘real’.
• Virtual is assumed to refer to properties that are internal to the agent (or ‘somewhere else’ in a
hidden dimension), and are therefore intangible or purely informational.
Any number of aliases or alternative interpretations of the four spatial relationships are possible;
indeed, they are encouraged in effective communication for expressivity and qualification. However, the
more specialized kinds of arrow we introduce, the harder it is to reason about them directly. However, we
should also be cautious that informal association of linguistic metaphor also leads to confusions about the
appropriate classification of meaning under the irreducible types, as interpretation by metaphor is fluid in
human language [15]. What the four types enable is a basic generic form of reasoning about the meaning
of a graph based on what kind of subject it describes.
7

DISTINCTION INFO

EQUIVALENCE

DISCRIMINATION

( PHYSICAL ) SITUATION

CONTAINS

LEADS - TO

spatial

temporal

NEAR

EXPRESS PROPERTY

similarity/distance

attribute/name

( VIRTUAL ) INFORMATION

Table 2: A alternative tabulation of the arrow types as belonging to physical (exterior) and virtual (interior) realms.
Expressions may either denote equivalence of neighbours or group roles, or discriminants of nodes or what forms
sequences.

ROLE / MEANING

NODE TYPE

PROPERTIES

space/direction

ACTIVITY / ACTION

event

ephemeral/realized

timelike (process) agent

SUBJECT / OBJECT

thing

invariant/realized

spacelike (snapshot) agent

SUBJECT / OBJECT

concept

persistent/unrealized

virtual (role/intention) agent

Table 3: Agent node types in semantic spacetime. Virtual agents may be thought of as being situated or ‘interior’
attributes of spacelike or timelike agents.

2.3

Resolving link type ambiguities with 3 types: events, things, and concepts

There is potentially some freedom in how to represent information given the foregoing link types. Even
with the four SST categories there are some potential modelling ambiguities, so from a computational
perspective it’s helpful to eliminate these with an additional formality. The residual ambiguity can be
resolved by recognizing nodes as belonging to one of three meta-types, denoted with small Latin letters
(e,t,c), which are induced by the four link types. The result is a generic γ(3, 4) representation, where:
e Events (e) An event is a temporary or ephemeral phenomenon. Such happenings may persist or
change in time, by L EADS - TO which is a time like transition vector.
t Things (t) are persistent phenomena, physical or realized (manifest or reified) agents in what one
would call classical space. A thing is a persistent phenomenon but may be created or destroyed.
They are agents that behave like matter.
c Concepts (c) A concept is an invariant notion which cannot be created or destroyed. It belongs to a
virtual space of ‘unrealized’ or ‘potential’ characteristics that can only be materialized by attaching
to a physical material agent.

2.4

The combined Γ(N, L) 7→ γ(3, 4) representation

With the selections described above, we have a compact set of organizing meta-types that allow certain
basic inferences to be made, relating to process semantics. The typing immediate implies a few constraints
that tighten up modelling. We can summarize the design implications for these choices in a few rules.
1. Things may be contained but not expressed.
2. Concepts may be expressed but not contained.
3. Concepts become realized by anchoring them to things or events.
4. Verbs are dangling concepts without a subject or object to instantiate them.
5. Verbs that are anchored to subjects or objects (things) are events.
6. A state of being which is realized is an event (john is/was happy, there was a moment of happiness).
7. A state of being which is unrealized is a concept (happiness).
8. A type of thing which is realized is a thing (a specific animal).
9. A type of thing which is unrealized is a concept (a general animal).
Example 1 (e, t, c examples) We shouldn’t think of objects as having a single representation in a
knowledge space. Different aspects of a phenomenon have to be represented differently to obtain functional
semantics:
• Mark’s life is an event.
• Mark’ body is a thing.
• Mark’s character is a concept.
• Mark’s appearance is a series of events expressing observations of descriptive concepts.
Example 2 (What is real?) Which part of semantic spacetime does an idea belong to? Physical or virtual? In the debates over the meaning of quantum physics, many authors have questioned what ‘being
real’ means, e.g. whether we can say electrons are real if they don’t behave like large scale material
bodies. The use of ‘real’ is too loaded with ideological connotations to be useful. Adopting this terminology of γ(3, 4) avoids these ambiguous stumbling blocks. One can be more precise about what is ‘real’,
‘beable’, or conceptual etc.
Example 3 (Collectives or collective histories?) Our ability to group events and concepts into a single
namable entity “all bicycles” (a concept) from the evidence of many bicycle instances (things) is powerful
but misleading because it exists only at a snapshot in time. In general, a process or history is a storyline,
a conical structure of causal development rather than an encapsulation of things. We only reduce it to
a single node in order to give it a proper name. Proper naming induces “nodality”, but generalizations
may be time dependent.
In the spacetime sense, things or concepts can be invariants. Concepts may be said to exist yet be unrealized, i.e. they exist in the imagination, as a model or idea. Events are ephemeral but still realized by their
happening. The distinction between ideas that are ‘realized’ and ‘unrealized’ is a simple clarification of
the implicit metaphorical indirection we use all too frequently in natural language. Are we talking about
the actual object or event X or are we talking about the idea of X in some context, or all possible X?
Invariants can’t be contained except by other invariants
Example 4 (Many roles and contexts need different types) Consider the word ‘library’, which we might
use quite liberally in language:
• A library is a concept.
• The library is a thing.
• Library is a role for a building.
• Library is a location attribute for an event.
• The opening of the library is an event.
• A library contains books is a statement of concept.
• The library contains books is a statement of things.
• Library expresses attributes, old, well stocked, damp, at centre of city.
When there are multiple paths, the selection of a particular value in a multi-valued inverse is affected by
whatever semantics are applied to the arrows.

2.5

Strategy in semantic graph representation

Graphs as knowledge representations are typically understood either as classifications or ordered concepts
or as social networks:
• Taxonomy: a hierarchy of proper naming and sub-classification.
• Ontology: a network of proper naming, classification and rules.
• Ad hoc property graphs: an unstructured network of atomic items and their relationships. All items
are assumed to describe entities on a similar scale or level.
In recent years, the proliferation of graph models and Semantic Web technologies has encouraged users
to model nodes in a graph by arbitrary linguistic atoms. Any simple conceptual meanings are presumed
‘atomic’. The Semantic Web is best known example of a social network in the stigmergic sense, with
embedded order [6, 29, 30].
Nodes are given meaning with supplementary ‘ontologies’ [14]. The ontology movement is rooted
in the primacy of first order logic for computer scientists. The focus of an ontology is to specify and
share meaning, whereas the focus for a database schema is to describe data [31]. A relational database
schema has a single purpose: to structure a set of instances for efficient storage and querying. The
structure is specified as tables and columns. An ontology can also be used to structure a set of instances
in a database. However, the instances would usually be represented in a [possibly virtual] triple store,
or deductive database rather than directly in a relational database. Ontology acts as a formal straitjacket
because it only works if everyone agrees to it.
Trying to naively shoehorn natural language into a graph structure is unproductive. For example,
suppose we try to express In common speech, we regularly transmute different aspects of a description
into one another all the time without much caution. Linguistic inference makes the algebraic underpinning of natural language grammar difficult. The brain skips many pedantic typological conversion steps,
flattening out sequences as if coarse-graining steps by aggregation. For example, consider:
The computer (saves data to) disk.

(1)

This is the kind of relation one often sees in naive triple store models. The arrow ‘saves data to’ is bursting
with assumptions. It is so specific that it is unusable outside of this example. In terms of the γ(3, 4) types
we can compare it to an SST graph (see figure 1).
10

?

?

?

saves data

disk

computer
event
thing

thing

saving event

computer

disk

start location
end location
has code

contains

thing

executing
code

has content

disk sector
location

computer
program

expresses
value

data
concept

Figure 1: A triplet graph compared to an γ(3, 4) graph.

One could argue that the ‘data’ implicit in the upper link of figure 1 should be provided as an input to
the graph. Without a decomposition, this can only be done by extending notion of links to hyperlinks as
in Milner’s bi-graphs [1, 32]. However, this introduces basic ambiguities that are easily resolved by the
γ(3, 4) decomposition.
The dotted lines indicate the subgraph representing the triple link. We see that it is not a simple
substitution.
If we say a thing smells of perfume we are adding an implicit indirection from the concept of the smell
of perfume to the liquid perfume itself. A smell is not a liquid, but there is only only natural interpretation
of this muddling of proper names so no harm is done.

The cake (looks like) mark’s house

(2)

What type is “looks like”? The cake is clearly a thing. Mark’s house is a thing, but “looks like”
expresses a similarity. If we write “has the appearance of” is looks more like a concept attribute, which
makes more sense. However now we have the rule that only concepts (not things) can be expressed as
attributes. Mark’s house is not hanging off the cake, only the likeness or image of mark’s house, which is
a conceptual representation.
These considerations are pedantic, like formal logic, because these are the typological distinctions that
lead to precision. But then why not simply use logic? We are trying to avoid that level of specification by
having only four types.
Concepts may express other concepts.
Stubbornness (means) explanatory concept
A hammer (may be used for) hammering event/activity
” (may be used for) hammering activity

11

(3)

An example of something is an event or instance not a member of a group
How shall we decide the correct kind of link? When do we use express versus contains? The meanings
of these concepts are rooted in spacetime ideas.
London (makes) cars
Tractors (are supplied by) Massey Ferguson
Apples (have colour) green
Apple (is a) fruit
In each of these cases, the kind of objects connected are wildly inhomogeneous. Without semantic
homogeneity there cannot be homogeneous reasoning. Every new arrow relation. It’s not difficult to see
that many implicit steps are involved in each of these. Although we might say these abbreviated forms,
we imply many more steps. Clear the city of London does not make cars: some factory located there
contains a process of manufacturing and assembling. Making cars is a property of the factory. However,
as the container of the factory, the city somehow inherits that promise of making cars. On the other hand,
Sally is stubborn and Sally lives in London. This does not mean that London is stubborn.
How can we resolve these idiosyncratic anomalies in the reasoning? The key to unravelling these
issues is to homogenize phenomena as generic processes. What process transmutes cities into cars, machinery into companies, fruit types into colours, and fruits into fruit types? The specific relations are not
as important as whether something is a chain of events, an explanation of an attribute, or a part of a larger
whole.
If several nodes are joined by a common association, typically a property expression or group container membership, then we can infer an implicit equivalence through correlation to the third party. If we
have
A (is located at) X
B (is located at) X

(4)

we can assume that A and B are correlated somehow. However, this is likely lazy modelling. Is this really
an invariant fact, or merely an ephemeral meeting? The database is describing a misleading snapshot of
reality. Most likely A and B only met in a single event of a more dynamical scenario. X should not be a
location but an event.
John Williams (composed) Star Wars

(5)

The use of a verb ‘composed’ lead to an ambiguous typing. Is Star Wars the film, the soundtrack, the
brand, the franchise, etc. Is the composition a script, a piece of music, a clay model? Human inference
can make a good guess and derive meaning from this (perhaps not without some confusion in general),
but a machine algorithm relies on precise typing and cannot.
One could write explicitly
John Williams (composer has musical composition) Star Wars Soundtrack

(6)

This is then a reusable relation that implies two clear types: composer and musical composition Written
in this way as an invariant factoid, it misses the opportunity to be precise. We should, once again, turn to
events as specific instances (analogous to objects instantiated from classes in computering).
John Williams

composed the music for the movie Star Wars

”

(event has composer) John Williams

”

(event has composition) Star Wars Soundtrack
12

(7)

If we write the link more explicitly:
Star Wars the movie (has soundtrack) Star Wars Soundtrack

(8)

we notice that the link itself induces types between the end points as long as we choose link names
accordingly. Thus the explicit logical typing of objects is an unnecessary constraint as long as there is
broad consistency of naming. This simplifies the matter of search by random access when we are unclear
what we’re looking for.
event
then

get the party started

investigation
begins

then

Professor Plum murders Miss Scarlet
in the library with the bread knife
because she refused to marry him
example of

concept

involves

Professor Plum

involves
contains

murder by subtle knife

involves
event

concept
answers question

how

example of

Prof Plum & Miss Scarlett in the library

event

subtle knife

Miss Scarlet refuses to marry Professor Plum

kind of

used for

In the library

concept

cutting bread

concept

concept

murder

used for

knife

answers question

event

involves

where

cutting interdimensional rifts

refusal marriage

answers question

concept
concept
concept

involves

involves

involves

thing

concept

Miss Scarlet

involves

use of subtle knife

library

marriage
concept

what action
concept

why
concept

Figure 2: Describing a more complicated scene in terms of pure spacetime semantics is easier if we clearly describe
events as more than the casual abbreviations we use in language. This is analogous to the distinction between a class
or datatype in computer programming and a particular object instance of that class.

We use the name of an object and the concepts and events it is part of loosely in natural language. Indeed, this is one of the strengths of natural language, which is likely optimized for the implicit functional
capabilities human brain: many such connections can be left implicit and be filled in by inference. Part
of the goal of semantic spacetime is to help understand these matters.
How we handle a complex issue like ownership may be seen in figure 3. Ownership is a mixture of
concepts and things. The person ‘mark’ is a thing so can only express a concept. To form a collection
of things we need an entity that can contain them under a single umbrella. The estate of mark is thus a
thing, and in order to express the ownership, we need to express the concept of ownership as a property
of this collection. Since mark cannot (in a natural since) contain many other items that are possibly far
away from mark, we use the attribute of connecting ownership as an expression of mark’s identity (which
is a concept).

Figure 3: We use the name of an object and the concepts and events it is part of loosely in natural language.
Ownership is a mixture of concepts and things. The person ‘mark’ is a thing so can only express a concept. To form
a collection of things we need an entity that can contain them under a single umbrella. The estate of mark is thus
a thing, and in order to express the concept of ownership as a property of this
collection. Since mark cannot (in a natural since) contain many other items that are possibly far away from mark, we
use the attribute of connecting ownership as an expression of mark’s identity (which is a concept).

2.6

Directional inference in the γ(3, 4) representation

Spacetime operations are often associated with group transformations that form chains or closures. With
a concept of graphical scale, and the type separations of γ(3, 4), we can now attempt to make certain
inferences without any specific ontological rules.
The aspect of systems which is supported by Semantic Spacetime is the frequently neglected dual
importance of quantitative dynamics with qualitative semantics [33]. Process representations that encode
knowledge may have several interpretations. The most common understanding of computation is based
on the arithmetic conventions of rings and fields for addition and multiplication of real numbers. Our
universal adoption of standard conventions means that we rarely question to consistency or meaning of
these rules, yet they are fraught with many ambiguities, which normally only surface when faced with the
‘loose end of’ division by zero.
Our semantic model is relevant here too, as representations of computations may involve either physical interlinking of agents (e.g. agents collaborating on a chip die to perform a computation) or purely
informational maps recording conceptual models of the processes (e.g. symbolic algebras).
The fact that we can represent computations as process graphs should be no surprise. It is a special
case of expressing reasoning as a graph, from ad hoc stories ‘once upon a time’ to highly constrained
algebraic logics. The lesson, however, is that this is not as simple as chaining together arbitrary triplets
of named storage locations as in an RDF graph. The usual formalized notion of an ontology is of little
use too, since it is too specialized and once always needs to extend or replace it. An ontology can never
stabilize except by limiting data or constraining the allowed phenomena.
The types C, E do not ‘propagate’ indefinitely in the sense that L does. They are representative of the
snapshot one obtains by freezing temporal evolution in a fixed configuration. We thus need to investigate
the idea of scaling for inference. For instance, if we take an example and generalize or specialize it
(going ‘up’ in the CONTAINS or EXPRESS direction), does the generalization have the same properties as
the example? For example:
• Mark is human.

• Mark is tired.
• All humans are tired.
This attempted syllogism is clearly wrong, but the opposite direction may be true.
• All humans are annoying.
• Mark is human.
• Mark is annoying.
As pointed out by Couch [34, 35], the approximate notion of “might be true” or possibility links
is sometimes the best one can do when reasoning, There is no precise logical one hopes with formal
ontology. For example:
• If A contains B and A contains C, then B and C might be near one another on the scale of A.
• If an event E involves A and B, then A must have been near B on the occasion of that event.
• If A contains a collection nodes Bi should they inherit properties expressed by A?
• If a collection of nodes Bi are embedded in A → Bi → C, then all the Bi are symmetrical or
equivalent with respect to this particular process and may be considered part of a single supernode
with redundant elements.
Interpretations like these are involved in the processes of reasoning by deduction, induction, and
abduction, etc. The application of these ideas to unlabelled probabilistic transitions of Markov chains is
the way current diffusion models of machine learning attempt to ‘reason’ [18].

3

Graph-algebraic Semantic Spacetime

Based on the foregoing definition of nodes and links, we can say more about the properties of these
graphs. A graph may be represented structurally by a number of matrices, in particular the adjacency and
incidence matrices, which represent maps of locations and flow gradients for the process concerned.

3.1

Proper names as semantic coordinates

In knowledge systems, such as taxonomies, one attempts to give a unique name to concepts in a contextually appropriate way. The name, although intended as a true representation of its unique meaning is
often not unique in practice. In a coordinate system covering a region of space, in which one attempts
to label distinct locations, a proper name is a kind of semantic coordinate. Unlike numerical coordinate
systems, proper names are often multivalued, which leads both to opportunities and problems when reasoning. For a simple graph representation in which the characteristics of the node are the name itself,
name associations may be considered tautologies, since our ability to make distinctions relies on there
being observable differences.
The scaling of names to groups and regions is not altogether trivial, however. In statistical subjects,
the importance of node or entity distinguishability has long been known and was shown by Boltzmann
to be associated with the degree to which systems are able to exert a causal influence (free energy and
entropy concepts) [25]. This issue of unique identity affects the way we model concepts and things in a
graph, particulary when using spacetime as a model.
The concept of proper name interacts with the γ(3, 4) types. A graph node is associated with a distinct
identity. The numerical identifier of the node or semantic identifier may be associated with its ‘proper
name’, i.e. the collection of symbolic or numerical attributes that are expressed within or outside the
node, i.e. the union of interior attributes S and the set of links {E(ni )} of type ‘EXPRESS PROPERTY’. In
semantic spacetime, scaling allows us to take an entire book of text as a node in a graph. The proper name
of the node is the entire text of the book. The node can be decomposed into smaller parts in a variety of
ways to express the meaning of the entity.
• A book, considered as an entity, has its entire text as its unique identifier or proper name. We can
give it several aliases, such as a title, a cryptohash or an ISBN number.
• The concept of the book with its unique text is realized in many physical copies, which express
the concept of the book by reproduction. The physical books exists and can participate in different
events (see figure 6).
• Compressed descriptions of events, things, and concepts can be unified under the umbrella of a
name: ‘The Battle Of Britain’, ‘Tractor’, ‘hunger’. As patterns, we define names to be concepts or
virtual attributes, rather than physical realizations. Names are usually recognizable patterns, with a
variety of manifestations: in speech or writing, etc. It’s the content of the realizations that imbues
the name, not the mode of implementation.

3.2

Graph structures: appointments and loops

When several nodes point to a single node as their successor, we call that an appointed node [5]. This
leads to a local amplification of flow into the node. Nodes that are pointed to are also called ‘hubs’, while
nodes that point to many are also called ‘authorities’ in social graphs [36, 37]. An appointing node is the
equivalent predecessor or source for several successor nodes. This is a division of the flow through the
node into weighted distribution, according to the link weights. Appointing and appointed nodes correlate
their appointees implicitly, and thus form a common dependency in reasoning. Such nodes are important
for several reasons. Pragmatically, they a absorbing nodes and therefore lead to division by zero issues.
Graphs may contain structures that have both semantic and dynamical consequences.
• Sources and Sinks: these are nodes that start and end a path through the graph. They exchange
places if one changes the sign of the link type.
• Appointed nodes: when several nodes point to a single hub that appointee is called an appointed
agent in Promise Theory. The cluster of nodes all pointing / electing a single individual are thus
correlated by the appointee (they have it in common). Such structures help us to see processes and
process histories.
• For ”leads to” arrows, these structures are confluences of arrows or explosions from a point.
• For ”contains” arrows, these structures are the containers or shared members
• For ”property expression” arrows, these structures are compositions of attributes or shared attributes common to several compositions
• For ”near” arrows, these structures are synonym / alias / or density clusters
Appointed nodes that are themselves appointed recursively form nodes that are called ‘central’.
• A central node for ‘leads to’ has a high level of involvement, implying a high mass for flows.
• A central node for ‘contained by’ arrows is the container.
−CONTAINS

{ni } −−−−−−→ n
3.3

Propagation and terminating absorption of the γ(3, 4) types

A node that propagates may form chains. Nodes that ‘absorb’ arrows are natural endpoints of graph flows.
Events are naturally propagating, without any necessary end. Both concepts and things are absorbing
types of node because there is a most primitive attribute in practice.
The mapping between directed graphs and sequences of events (as ST-1 ‘leads to’ arrows) creates an
obvious geometry for processes in a graph. These arrows become associated with proper evolution of
states, e.g. Hamiltonian evolution in symplectic systems.
Consider examples for how each of the 4 types propagates.
• A terminating L chain sequence is one in which there is no natural followup in the narrative:
(gestates into)

(becomes)

(flies to)

egg −−−−−−−→ caterpillar −−−−−→ a butterfly −−−−→ tree

(10)

Notice that ‘a butterfly’ is really the event of changing into the state of being a butterfly, which is
an event, since it wasn’t a butterfly before. Similarly ‘tree’ is really a shorthand for ‘the visitation
of the butterfly to the tree’, which depends on the material thing ‘tree’, but is not the tree. These
language subtleties trip modellers up frequently. The event may refer to a thing, but it is not the
same as the thing. If we neglect to make these distinctions, and believe too literally the meaning
of our abbreviations, we fall into the inconsistent the semantics of many knowledge graphs. Leads
to chains have no obvious end, unless we choose to restrict them, e.g. to focus on a particular
butterfly that eventually dies or transforms back into its constituents. Ideally, we would write the
events more clearly (see figure 2),
• A C chain:
(owns)

(made of)

(contains)

Mark −−−→ car −−−−−→ atoms −−−−−→ quarks

(11)

• An E (or P ) chain:
(has prop)

(has prop)

(has prop)

(has frequency)

(has units)

diagram −−−−−→ visual −−−−−→ colour −−−−−→ blue −−−−−−−−→ f −−−−−→ Hz

(12)

• An Nt chain:
(looks like)

(sometimes confused with)

(resembles)

Horsefly −−−−−−→ a butterfly −−−−−−−−−−−−−→ a moth −−−−−−→ angel.

(13)

The types of things now enter into these chains of reasoning.
The metaphorical text of the song ‘Love is Like a Butterfly’ can clearly be written as a simple triplet:
Love (is like) Butterfly. However, this doesn’t quite work in this more stringent representation:
(is like)

Love −
̸ −−−→ a butterfly,

(14)

because love is a concept and butterfly is a thing, and these cannot be alike while retaining this type
distinction. Cleary natural language works by metaphor much of the time, rather than by logic, and our
brains are quite good at seeing through these inferences. This, on the other hand, makes the challenge
of more careful logical explanation from natural language a non-trivial challenge. Arguments may be
presented for keeping or eliminating metaphor in graphical knowledge representations. As long as we
want a more formal calculative framework for reasoning (which is typically what we ask of machines),
the shortcuts of natural language are probably best avoided unless we can find a method for replacing
simplistic arrows with subgraphs that may be substituted in their place. In a sense, this is what Artificial
Neural Network representations are doing on a probabilistic level.

e
+
t
+
+
c

Figure 4: Allowed semantic transitions between node types, by kinds of arrow. There is a separation between virtual
or conceptual states and physical or material characteristics. Events are the encapsulating class for both of those.

3.4

Allowed type transitions of γ(3, 4)

We can now summarize the algebra of rules for valid graphs in the γ(3, 4) representation. See figure 4
and table 4.
Due to linguistic inference, once again, it seems that an event ought to be able to express or refer to
another event as an attribute, e.g.
Party celebrating the Olympics (refers to) the Olympics

(15)

The Olympics was an event and the party is an event. However, if we seek a clean distinction we need
a more pedantic eye. The use of Olympics in this case refers to the collective memory of the event, not
the actual happening event itself–which is a concept without physical manifestation. We might prefer
to mention Olympics only once in a knowledge graph, but then we would render all uses of the word
equivalent, which is semantic nonsense. The same type of reasoning can be applied to explain why things
cannot be expressed as properties. For concepts that seem to express things, e.g.
fast food hatred (is about) fast food,

(16)

we observe that the reference to ‘fast food’ is not a reference to an instance of fast food, but rather a
reference to the whole class of things we call fast food, which is a concept. Our minds quickly create
short cuts through these matters, yet we are also intuitively aware of the distinctions. This underlines
T RANSITION
e (±L) e
e (±C) e
e (Ne ) e
e (+C) t
e (+E) c
e (+E) c
t (−C) e
t (±C) t
t (+E) c
t (Nt ) t
c (+E) e
c (−E) e
c (−E) t
c (E) c
c (Nc ) c

EXPLANATION ( EXAMPLE )
An event can be followed by or lead to another event
An event can contain or be part of another event
An event can be similar to another event by any criterion
An event as a region of spacetime can contain a thing for its duration
An event can express a property or concept (timestamp)
can event can express a property or another event (celebration of Christmas of 74)
A thing can be part of an event, but an event cannot be part of a thing.
A thing can contain or be part of another thing
A thing can express a concept as an attribute (blue car)
A thing can be close to or like another thing
A concept can refer to an event as an attribute (that one time at band camp)
A concept can be an attribute of an event (a time of happiness)
Concepts can only be attributes expressed by things (blue car)
A concept can have properties or be a property of something else
e.g. (blue is a colour)
A concept can be similar to another concept, (aquamarine,turquoise)

Table 4: Explicit transitions allowed for events, things, and concepts through the four link meta-types.
(as is well appreciated) that, without quite sophisticated automated analysis capabilities, natural language
could not be understood literally. Our penchant for metaphor is busily at work in natural language [15].
The consistency of these relations can be show using the matrix algebra in section 4. In short, we can
use this decomposition to define what are events, things, concepts. One could add more detail, but we are
trying to compress description into simple elements rather than exfoliating.

L
C
E
Ne

e

C
E
Nt

t

L
C
E
Ne
C
E
Nt

E
c
Nc

E
Nc

Figure 5: Allowed semantic transitions through node types, by kinds of arrow. Not all of the links are freely joinable,
however, so there are restrictions on allowed transitions.

3.5

Inference rules and symmetries

In an unconstrained graph there can be no rules for inference, because rules require some regularity and
functional predictability. In Semantic Spacetime, however, one has four broad kinds of relation and three
kinds of node or entity.
If we seek a strongly constrained deterministic logic, as in ontology approaches to graphs, the result
will be either simplistic or intractable. Inference about common properties are straightforward, though
remain speculative.
• If a collection of nodes ni are contained by a node nC which expresses property P , then one might
infer that the contained nodes might also have this property.
• If a node has a property P and is declared to be similar/near another node, then the similar node
might also have this property.
This possible inferential reasoning goes back to the discovery by Alva Couch in join work [34, 35],
which was subsequently deepened in the development of semantic spacetime. Unlike logical ontological
schemas, this kind of reasoning is simpler but inexact. With ontological first order logic, results are either
precise or non-existent. In practice, most relations on data that are not carefully designed will fail to yield
any result due to the over-constrained nature of first order logic.
A second kind of symmetry concerns patterns of inference in the linkage of nodes. Duplicate nodes
may arise in a graph either by accident or by a deliberate encoding of redundancy (degeneracy). If a
collection of nodes each possesses the same incoming and outgoing links of a given ST-type, then we
can infer that they are functionally equivalent with respect to that process. This implies that they can be
treated as a single node (which we refer to as a supernode) for those intents and purposes, though perhaps
not all. While the nodes might have the same links for, say, causal trajectory (‘leads to’), they might have
different properties. This suggests another possible set of inferences or warnings to flag: why is there a
partial but not complete equivalence? Is it intentional or accidental. Is it, in fact, an error?

3.6

Absorption by blind alley and by statistical aggregation: scales and degrees
of freedom

How do we draw a ring around a region of interest or influence in a graph? The role of an agent boundary
in determining what is a value or operation in a given context is important to many of the concepts as
we scale up or down a hierarchy of meaning. Concepts emerge by recombination of atomic or genetic
concepts and attributes, suggesting that C, E spatial types have semantic limits. L may have frequently
have practical limits due to the ephemeral nature of interactions and processes in general, but there is no
obvious upper limit to the extent of time.
The structure implies by the SST γ(3, 4) model is of a graph composed of snaking causal sequences of
events or ST-type 1 (‘leads-to’) connected nodes, where each node has ‘contains’ and ‘express property’
nodes in orbit around them, expressing their interior attributes. These may themselves be in orbit around
nodes that contain or express them. A few ‘nearness’ links provide shortcuts (wormholes) between nodes
that are marked as being similar for reasons outside the scope of explicit causal process knowledge.
One can say that events effectively act as bipartite bridging nodes between invariant things and concepts, leading to a temporary connection in the sense that an event has a finite contextual validity or
lifetime even if the historical node remains in the graph.
Aborbing regions arise whenever there are source nodes or dead end sinks in a directed graph. Once
we descend into the details of a node, by CONTAINS or EXPRESS, we find their invariant attributes. They
live in an interior subspace orthogonal to the timeline of events. The timelike ST-type 1 (‘leads-to’)
are analogous to the Hamiltonian evolution in symplectic systems of physics. This orthogonal subspace
is analogous to the hidden dimensions of a Kaluza-Klein or string theory, a field space of forces or
‘interaction semantics’. The entire directional links for containment and expression point to ultimately
aborbing regions, as they are spacelike.
There is an erasure of information in two ways here:
• Flows that end up pooling at sink nodes are aggregated with a loss of distinguishable history or
identity.
• Processes that drive several nodes towards an appointed successor merge separate dynamical flows
rendering them indistinguishable. Conversely, a node that splits a flow to several successors with
a weighted distribution is either sharing (dividing) or amplifying (multiplying) the flow along the
links.
Clearly driving independent flows through a common node erases information and semantics, coarse
graining the result. If one cares to measure the incoming distribution against the outgoing, with an
entropy function, there is a change of entropy–potentially an increase or a reduction depending on how
one chooses to define the semantics of the result. One does not avoid process semantic issues merely by
adopting a conventional narrative.

event

event

event

storyline

thing
thing
thing

ideas about
thing

naming

Figure 6: The logical structure of events, things, and concept is subtle. Events happen in real exterior space. Things
exist in real exterior space, but ideas about things are interior to the agents that express them. They can become
shared by interaction (like entanglement of quantum agents) but they are a priori private on the interior of agents.

The significance of absorbing regions, sources and sinks, for a finite system is clear semantically.
Dynamically, however, there are different ways to interpret the processes.
There are certain elements that are, in a sense, atomic or irreducible by virtue of being a logical end
to a process. States with both incoming and outgoing arrows are transitory but reducible ultimately to the
sets of nodes either leading to it or emerging from it. In differential calculus one refers to these processes
as retarded and advanced solutions.
Equilibrium solutions are also possible by selecting from both sets of arrow, or removing the arrow
direction altogether. These are known as Feynman boundary conditions in physics. The latter is interesting because it introduces a second type of absorption for a process: a statistical absorption, which is
associated with entropy. Any bulk state of a system, which converges statistically by some separation
of scales into an average probability distribution of values, may be called statistically stable [17]. For
example, an ideal gas at finite temperature has a statistically stable (or maximum entropy) distribution of
particle velocities, which it makes no sense to count as distinguishable phenomena. This, at the semantic
level of the bulk gas, the thermal state is absorbing—and represents another kind of ‘zero’: placing a
small object into a large (fabled infinite) reservoir at temperature T0 leads to the process:
Equilibrium Tobject = T0 .

(17)

In other words, semantically, equilibration is another kind of zero operation.
Finally, we note that in artistic and data‐driven applications, the semantics of process
transformation and information loss at such absorbing nodes can inform strategies in
machine learning and optimization.

4

Matrix representations of graphs

A graph has an associated matrix representation for mapping the topology onto a linear map acting on
rings or fields. The matrix representation is a crucial
map between arithemtic algebraic rules and the
realization of processes as graphical structures.
The adjacency matrix A and its transpose AT are square matrices, whose rows and columns are
the node labels, and whose non-negative elements represent quantitative link weights. The numerical
values are typically set to 1 in elementary texts on graph theory, but they can have relative weights as
well as semantic labels. These may be chosen as non-negative real numbers. An undirected graph (with
arrows in both directions) is symmetrical about the leading diagonal, i.e. A = AT . A directed graph is
asymmetrical and its source and sink nodes, which are the starts and ends of paths, lead to zero rows and
columns, which consequently lead to zero eigenvalues of the matrix. This proves to be important in a
number of ways, as it implies the matrix is non-invertible, preventing predictive process-path reversals.
A complementary line or ‘join’ graph matrix is the complement of the adjacency matrix, where rows
and columns are represented by the links joining nodes. Both of these may be found from the incidence
matrix.
The incidence matrix and its complement describe the emission and absorption of link lines from and
to nodes. Since they are local to a single node, they may be associated with the promised intent of the
nodes concerned. The standard conventions in most texts are for undirected graphs and are unhelpful here.
Directed and labelled graphs with self-referential loops require a separation of the incident matrix into
two parts. These are analogous to the offer and acceptance promises in Promise Theory, and the matrix
elements factor from a Hadamard product form into two complementary matrices that are effectively the
square roots of the adjacency matrix: I (+) and I (−) .
Iˆ(+) Iˆ(−) = Â + Ĉ

(18)

for some diagonal matrix C belonging to the Cartan subalgebra of the flow. When there are no autoreferential
(pumping) self-loops, C is proportional to the identity matrix. The incidence matrices I (+)
and I (−) correspond to the Promise Theoretic offer and acceptance promise rates [5]:

Iˆ(+)

=

L

e 1
t0
c 0
e
L
1
C 
1
E 
1
Ne 
1
Nt  0
Nc 0


Iˆ(−)

=

C
1
1
0
t
0
1
0
0
1
0

E
1
1
1

Ne
1
0
0

c

0
0

1

0

0
1

Nt
0
1
0

Nc

0
0 
1

(19)

(20)

Notice that these are not simple transverses of one another, as they would be in a simple undirected graph,
since a thing cannot be expressed (a forbidden state transition). We distinguish nearness links for each of
the three types e, t, c for convenience.
Rates of change within the processes can be represented as derivative. The ‘dynamical’ graph derivative for node values is defined ∇i vj = vi − vj . This corresponds to the usual Newtonian derivative
∂x v(x) for a function which is distributed over graph nodes ⃗v (N ). There is a second notion of rate of
change for a graph: because links define both direction and value between each pair of nodes, they also
behave as a vector field, which has a gradient role of its own. The matric of links mapping to links forms
a dual ‘line graph’ representation in which there are rows and columns for every independent link, no
matter the nodes they connect.

4.1

Matrices for γ(3, 4) skeleton

The γ(3, 4) skeleton graph can be represented, without explicit nodes only type names, as a set of matrices
characterizing the graph. Certain rules about process semantics mean that transitions between certain
node types are limited to specific kinds of arrow:
e
e ±L, ±C, ±E, Ne
t
−C
c
−E


Â = A(ni 7→ nj )

=

t
+C
±C, Nt
−E

c

+E
+E 
±E, Nc

(21)

This can be decomposed into a number of generators with antisymmetric (or anti-Hermitian) signatures:
e
e ±1
t 0
c
0

t c

0 0
0 0
0 0

e

e ±1
t  −1
c
0

t
1
±1
0

c

0
0
0

e
e ±1
t 0
c −1

t
0
0
−1

c

−1
+1 
±1



ÂL

ÂC

=

=



ÂE

ÂN

=

=

e

e 1e
t0
c 0

t
0
1t
0

c

0
0
1c

(22)

(23)

(24)

(25)

Conversely, arrows in a trajectory can only be joined by certain types of node as a path join matrix J:

Jγ(3,4)



a′


−
→ ni −→
a

=

L
C
E
N

L C
E
N

e
e
e
e
 e e, t e, t
e, t 


 e e, t e, t, c e, t, c 
e e, t e, t, c e, t, c


• A process L must terminate on a final event or never.
• A property attribute process E must terminate on an atomic concept (property).
(26)

4.2

Stable regions of a graph, information, emission and absorption

Particularly for directed graphs, but also for some undirected ones, an important feature within a graph
is the number of nodes at which the link flow converges (when absorbed by a node) or diverges (when
emitted by a node), i.e. for in- and out-degrees greater than 1. These are confluence and branching points
in the arrow vector field, which are literal and metaphorical singularities. As remarked in [38], emission
and absorption by nodes is associated with zero operations.
There are certain operators which transmute interior state to exterior movement in the graph. These are
related to ladder operators known in the context of differential equations, and they are further connected
to absorbing states of the graph. Convergent semantics with idempotence of an operation one some end
state comes about from ending up in a cycle that has the identity element as its final state. This is used to
good effect in enforcing policy choices [38]:
Ô⃗v

= ⃗v0

Ô⃗v0

= ⃗v0

(27)

Consider figure 7: We can write this algebraically as:

1
0
2

3
Figure 7: A convergence of flows at a point is an absorbing region of the graph, associated with a singularity.

Arrow n1

= n0

Arrow n2

= n0

Arrow n3

= n0 ,

(28)

which is clearly isomorphic to
0·1

=

0

0·2

=

0

0·3

=

0.

(29)

in other words, by virtue of ending up at the same state ‘0’, the arrows lead the process into a location
which does not remember the route by which is arrived there. If there are no arrows flowing away from
node 0 then the node is completely absorbing (a kind of black hole for the process), and it is called a sink
node. The adjoint, in which arrows are reversed would be a node that emits arrows from nowhere, called
a source. Even with arrows both incoming and outgoing, any such multi-line convergence is singular, and
indeed this is represented in the adjacency matrix of the graph by the existence of zero eigenvalues–and,
indeed, the implications for invertability of the zero operations. Inverse of zero is a topic that several
authors have discussed.
A hub operation of this kind turns a distribution into a single value and vice versa. Clearly, one can
turn n values into one by averaging or some selection process, but turning one value into n requires
more information. One can duplicate, triplicate, etc values to send out identically to multiple redundant
destinations. Then the distribution of values is unchanged. Alternatively, one can use a new source of
information in different directions to determine the result in different directions. One possibility is to use
a differentiated distribution of link weights. However one chooses to inject information, it has to come
from outside the starting node.
With multiple states distinguished semantically, rather than representing different locations on a neutral
number line, we now have more responsibility to define the degrees of freedom carefully. In physics,
one uses entropy concepts to describe and measure the extent and homogeneity of a distribution over
states. The Shannon entropy [39,40] is defined over some distribution of states partitioned into N choices,
and measure each with an alphabet of states 1 . . . C.
X
pi = xi /
xi
(30)
i

S

= −

N
X

pi logC pi .

(31)

This is maximal S = logC N , when pi = 1/C, ∀i, and minimal S = 0 when pi = 1 for some choice.
If we increase the resolution or alphabet of the distribution N → ∞, then the entropy gets larger. If
we coarse grain by making C → ∞ then the entropy approaches zero and then ceases to be defined for
normal arithmetic rules [41]. The ambiguity lies in how we count the implicit dimension of the state space
(see figure 9 and the discussion below). The issue here is that a complicated process has more attributes
or ‘degrees of freedom’ than a simplistic view of the number line.
In thermodynamic statistical physics, the Boltzmann entropy mimics the Clausius entropy as a measure of the energy in a system, which by virtue of being distributed indistinguishably around the system,
has lost its capacity to do work. This is because statistical absorption is just as powerful as absorption by
a single state or node of a process. Conservation of information in a process is thus inherently limited
by the nature of absorption in the network.
4.3

Sources and sinks

A state is absorbing in a semantic graph because this reflects the interpretation we intend for it. In other
words, it’s no accident that we end up with absorption. The same is true in arithmetic if one is careful,
but there are cases where one is led to seek answers in ways that bump into problems concerning the
incompleteness of definition.
If we think if the values in the field as nodes in a graph, with arbitrary many nodes, then operations
that take us from one value to another may be represented as links with particular arrows that represent
the operational semantics.
Graph transformations are used widely in machine learning diffusion models for image reconstruction
and enhancement. They are also of interest here in a process knowledge representation for tracing the
contextual relevance of the map, which I’ll return to below.
Consider nodes 0,1,2 in the confluent junction in figure 7. The partial adjacency matrix is:


0 0 0
(32)
Â =  1 0 0 
1 0 0
From this, the transposed adjacency acts as a forward stepping operator over the vector landscape of internal node values, and the untransposed matrix is a backwards stepping operator when acting on internal
graph node values, represented as a vector ⃗v T = (v1 , v2 , . . .) :




0 f1 f2
0 0 0
ˆ
ˆˆB =  b1 0 0  ,
ˆF =  0 0 0 
(33)
0 0 0
b1 0 0
so that


0
F̂ ⃗v =  0
0

f1
0
0


 

f2
x1
(f1 x1 + f2 x2 )
.
0   x2  = 
0
0
x3
0

(34)

We note that B̂ and F̂ are not inverses of one another, since they both contain zero eigenvalues, i.e. have
determinants of zero. Operating on the graph’s internal state with this stepping operator, we see that the
values from nodes 1 and 2 are shunted onto node 0, with the weighting determined by the adjacency
matrix link weights. The original value at node 0 falls off the end of the absorbing node into the void and
is unrecoverable. If we now try to reverse this in order to restore the original information, we see that the
absorbing node wipes out the memory of the system rendering an inverse impossible without an input of
new information:

 


0
f1 x1 + f2 x2
0 0 0
 =  b1 (f1 x1 + f2 x2 )  .
0
(35)
B̂ F̂ ⃗v =  b1 0 0  
0
b2 0 0
b2 (f1 x1 + f2 x2 )
If we select the values for f⃗ and ⃗b appropriately, we can partially restore the original state, but not without
specific knowledge of the original configuration and the ability to inject appropriate values into other
nodes. The absorbing node x3 ’s value is lost forever unless we insert a value by hand (as a matter of
policy). and the initially unused source value x1 has been injected. The values of b would typically
involve division by the dimension or node degree ki n of the absorbing junction node 0.
(f1 x1 + f2 x2 ) → 1, b/f →

1
.
2

(36)

Why is the reverse operation not equal to the inverse matrix? This can be traced to the zero eigenvalues
(and zero columns in the F̂ operator). The computation of a direct inverse would require a division by
zero, which could yield any value, from eqn (29).
For a 3×3 matrix we can write the inverse explicitly for arbitrary real numbers a, b, c, d, e, f, g, h, i, j:


a b c
M = d e f 
(37)
h i j
The transposed cofactor matrix (adjugate) forms the the linear combinations which can be reverse engineered to yield cancellations or determinants, giving a formula:


(ej − if )
−(bj − ic) (bf − ec)
1
 −(dj − hf ) (aj − hc) (af − dc) 
M −1 =
(38)
det(M )
= a(ej − if ) − b(dj − hf ) + c(di − he).
The difficulty arises in the scaling of the values to renormalize the diagonal to ⃗1:
M M T = M T M = I = ⃗1.

(39)

For a sparse graph, most of the values a, b, . . . are zero. The ability to evaluate the expression is then
unclear, since the rules for rings and fields do not admit division by zero. Here there are so many zeroes
in multiplication that one has to see them as strings of operations rather than mere numbers.
The renormalization of the inverse is related to an overall factor of the determinant of the matrix. Since
the determinant is the product of the eigenvalues, it is zero if there if a zero eigenvalue, which corresponds
to a zero row or column. For a directed graph, this corresponds to a node which goes nowhere i.e. an
aborbing state.
The effect of B̂ F̂ is to eliminate the degree of freedom at x0 and to coarse grain the values of the others
so that they become equal. The only way to restore the original distribution is to encode the original values
by a memory operation M̂ (⃗x, F̂ ) → B̂. Indeed, this is schematically how diffusion models of machine
learning restore images: by training an inverse process to capture the process memory of destroying the
image through insertion of noise. The presence of 0−1 in the determinant or inverse indicates the need to
remember past history, a snapshot of the past to ‘roll back’ to, as pointed out in [38].
We can now associate the semantics of stepping and state exposure with the semantic spacetime
properties.
The same inverse notions apply to the semantic graphs. If we try to shift context up or down a branching graph, in the information hierarchy, crucial context may be lost. The relevance of the path is reduced
by the dimension of the possible alternative pathways. This loss is happening inhomogeneously all over
the graph where a process is ongoing. While this might initially seem harmless, making sense of the result
requires a continuous input of new information to keep the inverse on course–but, on course to where?
Deciding this selection requires an intentional act, i.e. an intentional insertion of policy information at
each stage. In diffusion models, this is provided by prompt information from a user.
As with logics, if one attempts to remember complete and precise information, then one would only
be able to generate results that were explicitly input. No recombinative mixing or ‘lateral thinking’ could
enter the process to create something new (however derivative).

4.4

Frobenious-Perron eigenvector theorem

The eigenvector equation
M⃗v = λ⃗v ,

(40)

for some matrix M and vector ⃗v has solutions called eigenvectors, which are intrinsic (eigen) properties
of the matrix in some sense. The equation can be applied to graph matrices by taking the adjacency matrix

M 7→ A, which is non-negative, over the nodes of a graph Γ(N, V ) 7→ (⃗v , A). This technique is widely
used in social network analysis to perform so-called importance ranking of nodes in the graph [37].
The Frobenious-Perron theorem for non-negative matrices states that the largest or principal eigenvector of any such graph will be entirely positive. More significantly, the semantics of this vector attach
to the eigenvalue equation: Iterating equation (40) represents a recursive propagation of node values over
links, weighted by the link values, which implies that multiplying any non-zero vector repeatedly by A
will converge ⃗v towards the principal eigenvector.
The purely positive addition of purely positive values must yield the highest eigenvalue. An undirected graph is in flow equilibrium, so there is no net direction to the movement of values over the links.
The equilibrium distribution of ⃗v in the principal eigenvector thus represents the reservoir water level at
each of the nodes (flow capacitance) at equilibrium. In social networks, this is equated with social capital
or ‘importance ranking’ (an important person is someone with many important friends).
For an undirected graph, where node connections propagate without opposition, all the value flows
along directed paths to sink nodes, where it pools. The normal algorithm for computing the eigenvector by
operating many times with A onto a vector of ones ⃗1 fails in this case to give the right answer, essentially
due to the ambiguity of division by zero. When there are zero rows (absorbing nodes) the effect of this
computation is simply zero. A more careful analysis shows that the vector components for absorbing
nodes should be non-zero as this is where all the flow piles up, but the multiplication by zero overwhelms
the normalization of the vector by λ. which may be zero itself for the absorbing nodes yielding 0/0 7→ 1.

5

Graph semantics of operational arithmetic computation

We can illustrate some of the semantic choices in basic arithmetic operations using graphs as the domain
‘space’ to model what we mean by the operations. The standard meanings are so ingrained in us from an
early age that it might seem strange to question them, yet doing so is quite instructive. The purpose of
doing so is not to necessarily change conventions or solve some inconsistencies, but to point out how a few
common themes trace back to interpretational ambiguities that we take for granted in mundane arithmetic.
Without adopting the standard jargon of rings and fields, arithmetic concerns the rules for counting
and measuring amounts of ‘stuff’. It uses formal quantities called x to model ‘amounts’ and handles
operations of augmenting and combining (+), depleting (-), duplicating (·) and sharing (/) operations.
These operations should work in both quantitative and qualitative interpretations. The abstractions either
expose or conceal information however.
In the algebra of rings and fields, the status of numbers as positions or as shifts is blurred into a single
set theoretic domain along the number line. Apart from this conventional interpretation, the question of
semantics remains ambiguous for graphical representations of operations. Should numbers be thought of
as value locations or as part of transformational operations? Should we treat numbers as events, things,
or concepts? And perhaps more pertinently, do we muddle these interpretations carelessly in dealing
with numbers? This question is particularly interesting in the service of Quantum Theory, where operator
algebras and numbers co-mingle extensively.
As an illustration of this, we can begin by looking at graph representations of basic arithmetic reasoning and ask what are natural interpretations for addition, subtraction, multiplication, and
division where the domain of mapping is no longer simply a single copy of the number line. Although this feels slightly
self-indulgent, it’s illustrative of the fundamental issues in attributing semantics to formal processes and
therefore underpins everything else in a form which is familiar to all readers. Let’s consider some examples, which might not be exhaustive.
The first question for semantics is to ask how we actually mean to represent a number to be added,

subtracted, multiplied or divided. There is no unique answer to this question, but we can naively imagine
people counting pebbles or abacus beads. The standard rules for calculating are guided by a principle of
closure around a set of numbers: when we combine numbers, the result should be a number of the same
‘type’.
+ : R × R 7→ R.

(41)

This is clear enough in the limited realm of mathematics, because the semantics of the ordered number
line R have already been defined (see figure 10). The number line is a key visualization of the process
that works well for a geometrical interpretation of addition, but less well for a bulk interpretation. In our
standard definition of division, for example, which is based on the primacy of the number line, there are
‘design issues’ to consider: if we wish our answer to a division operation to remain (mapped back into)
the scope of the number line, then we can no longer both map an agent of size x back to an agent of
size x and split the original inventory amount into a parts for all values without expanding the concept
of numbers to include fractional amounts. Once one introduces new numbers, negative numbers and so
forth, the consistency of the whole is jeopardized unless one can close the
√ operations convincingly. This
had resulted in the invention or discovery of symbols representing i = −1, and more recently ⊥ or Φ
for concepts including 1/0 [42, 43].

5.2

Special arithmetic states 1 and 0

Key to establishing the axioms and theorems of rings, fields, and groups are the special values 1 and
0. These binary values have attained legendary status in the digital age, but their significances are more
important than binary arithmetic. They are the two stable fixed points of arithmetic:
1·1

=

n·1

= n

n·0

=

1
0

(42)

These are graphical processes (see figure 8) Here there is a special value 0 which is an absorbing state,
i.e. arrows that enter do not leave (figure 9)).
0×0

7→ 0

(43)

1×0

7→ 0

(44)

2×0

7→ 0

(45)

3×0

7→ 0

(46)

4×0

7→ 0

(47)

...L × 0

7→ 0.

(48)

In this interpretation, the proposal that x/0 7→ ∞ is a statement of the dimension or degeneracy of the
inverse map. The inverse map is not single values and thus a prescription for interpreting it is needed.
In machine learning diffusion models, for example, this inverse is interpreted as a Markov process and a
policy decision is used to invert the map to get ‘something from nothing’.
More generally one can introduce a new value (something like the square root of minus 1) such that
x/0 ≡⊥. But what kind of object is ⊥? In the finite graphical interpretation, one might imagine that the
dimension of the result would be L.

5.3

Addition as a graphical process

Translation is one dominant interpretation for additive arithmetic. Consider the expression:
x′ = x + a

(49)

where x, a are what we intend to mean as ‘numbers’ (counts or measures). This operation of addition has
two common interpretations that are easily distinguished in terms of agents. In order to use agents, we
only need to assume that the set of agents is countable. In this model a number may refer to:
1. A location of an agent on the number line R, which is a externalized total ordering of agents according to their number proper identities. The meaning of addition is then to translate or redefine the
labels from coordinate position x to position x+a. Relativity has implications for the interpretation
of these operations too.
2. The amount of interior holdings of the agent of some counter (e.g. money or energy etc). The
meaning of addition is then an augmentation of the holdings from x to x + a. We say this is interior
to the agent, because its position hasn’t changed in our new interpretation. However, one could
also argue that this is the same as introducing additional exterior dimensions that are private to
our invariant meaning for location (as one does in Kaluza-Klein or string theories of physics, for
instance). The question of where new things come from, or how the semantics of a (an increment)
differ from the semantics of x (a state) is typically brushed aside.
The difference between interior and exterior ‘locations’, allowing us to distinguish exterior location from
interior holdings clearly has ‘boundary value’ implications.
Indulging the forbearance of the reader for a moment, let’s examine this in more detail, since the
issues are central to semantics of space and time and all graph based knowledge representations. We take
the first of these additive cases whimsically to mean a translation along the number line, as in figure 10:
In most cases, we think of x and a as being representative of numbers belonging to the same set, not
to different copies of a set that looks like R, yet this is misleading if not inaccurate. In both cases, the
closure of the rational or real numbers under addition is a formal identification of outcomes back onto the
same space, so while we write (41), we actually intend:
+ : R 7→ R × R 7→ R.
(50)

In a geometrical perspective we can think of x as a position, and a as being a shift that moves a marker
from x to x + a.

5.4

Subtraction as a graphical process

Subtraction semantics x − a imply taking away part of a measurable state. Once again, our interpretation
will depend on how we give meaning to x itself. As a translation, subtraction is essentially the inverse of
addition. However, if x refers to an inventory amount, then subtraction implies a removal of that
inventory. The handling of values near or at zero is especially delicate and may invoke
concepts like debt or borrowing. In many cases, subtraction is reinterpreted in terms of adding the negative,
thus preserving group theoretic properties.

5.5

Multiplication as a graphical process

Multiplication addresses the duplication, triplication, and n-ification of an amount. It also plays
a role in the notion of repeated addition. One sees both interpretations in mathematics: as repeated
addition and as an operation reflecting the Cartesian direct product, expanding the number of
entities or agents. As with addition, multiplication can be understood either as an external translation
along a numerical axis or as an internal scaling of an agent’s inventory. The dual interpretations
highlight the tension between value replication and aggregation.

5.6

Division

Division is conceptually the inverse operation of multiplication but presents challenging
semantics when representing processes. Division may be thought of as sharing an amount equally among
agents or partitioning a process into parallel parts. However, when the divisor is less than one or, in
the extreme case, zero, traditional arithmetic breaks down. In process terms, division involves reassigning
or redistributing inventory across a new set of agents, thereby raising issues of conservation and
information loss. In many cases, division is defined as the average share value, but this averaging
conceals the richer dynamics of flow distribution.

5.7

Division by zero semantics

The more fraught issue of division by zero must also be handled by a more careful convention. Its
interpretation is intimately linked to the nature of absorbing states in a graph. Dividing by zero might be
viewed as an attempt to share an amount with no recipients, leading either to an infinite requirement
of supplemental information or to a null result. In Promise Theory, this can be seen as a scenario where
an agent offers a value but no agent is present to accept it:
+x

x · (0) : x −−→ {}

(57)

Thus, division by zero challenges our conventional arithmetic and demands either the introduction of
special symbols (e.g. ⊥) or a redefinition of the process in order to preserve consistency.
In certain algebraic frameworks, such as those of Fock space or Lie algebras, the treatment of the zero
state in conjunction with annihilation operators provides an analogy: zero, though absorbing, can be
transmuted under controlled conditions into a finite value.
Overall, while traditional rings and fields strive for closure under arithmetic operations,
the nuances of division—especially when zero is involved—illustrate the limitations of standard algebraic
models when confronted with dynamic, process-oriented semantics.

6

Conclusions

Building on the Semantic Spacetime model as a set of guiding principles for graph representations, we
can simplify the selection of proper link identification by adopting the γ(3,4) representation to remove
unnecessary ambiguities, without adopting ontology or a detailed first order logic. The graphical properties of the algebra are then postulated to be compatible with all expected processes and inferences can
now be written down explicitly and be verified by matrix algebra.
It remains for future work to understand how to understand whether knowledge is trustworthy [52,53]
by the balance between the different node etcs e, t, c affect the reliabilty of a knowledge representation.
If a knowledge structure contains only concepts, it lacks grounding in truth and can easily disconnect
with reality. Could this explain what we are seeing with artificial intelligence knowledge, fake news, cult
beliefs and extremism? This is a problem of interest for a more empirical review.
Technically, the presence of absorbing states is inevitable in graphs with non trivial adjacencies. These
absorbing states need to be interpreted carefully. If we consider the graphs to be dynamic flows, they lead
to loss of information at the edges of the system. The wider problems of consistent transformations within
a finite structure have a common theme: the absence of invertability, due to states that are absorbing. In
Markov processes, there is no memory to prevent loss of information, but one assumes that information
is redistributed in such a way that it isn’t actually lost. Absorbing states behave in a similar way, until the
moment we want to reverse flows that lead to them. Remedies are analogous to the remedies for division by
zero. It would be interesting to study the effects of the various remedies on matrix inversion for
directed graphs in more detail.
Axioms and logical primitives already stand alone as ‘boundary information’ or irreducible knowledge. Such boundary information is thus intimately related to the absorption and emissions of nodes in a
directed graph. As one uses the SST structures to scale structures, such features will continue to exist on
many scales. Whole regions of a graph can be absorbing. Hierarchical graphs, such as taxonomies and
spanning trees, also have nodes that are both the beginning and end of some flow. Finiteness demands
this. Only in quasi-continuum models, groups and semi-groups, are we able to define ‘translationally
invariant’ systems that go on and on in different directions. So the infinity of possible outcomes for end
states avoids dealing with beginning and end states. This absence of boundary is used frequently in physical models such as field theories, to argue for smooth continuity. Clearly, the concept of zero and infinity
are complementary in this process sense.
There are no moving bodies in the Semantic Spacetime discussed here, however intrinsic properties
of space can be passed along to relocate from node to node or from agent to agent. This is called Motion
Of The Third Kind [1, 54]. In a sense it is forced to ‘invent’ the semantic split between matter and
spacetime in order to resolve ambiguities about changing states, as natural philosophers found necessary
in the formative years of physics. The semantics of material and conceptual constructs are similar. It’s
also reminiscent of the arguments about whether one should call interior properties real or not, which
continues to rage in the field of Quantum Mechanics.
In modern Artificial Intelligence or reasoning models, where many of the techniques originate from
the palette of mathematical physics, there is a tendency to paint path selection and inference with the
broad brush of probability theory, without questioning too much what the probabilities mean. The labels
that can remember inverses have to be trained explicitly at some expense in shadow representations of
knowledge. The relatively recent forays into context and relevance modelling are suprisingly overdue in
artificial neural network representaions. Semantic Spacetime belongs to this effort. Semantic spacetime is
not a natural language model. Natural language remains a very different way of compressing intentional
descriptions with rich semantic content, which undoubtedly relies on the capabilities of an evolved brain
for bridging the gulfs between broad and sometimes audacious inferences. The possibility of scanning
natural language and compiling a compact SST representation is nevertheless an intriguing possibility
of great interest in connection with generative Artificial Intelligence, with or without Large Language
Models.
A software implementation of this work is available at [55].

References
[1] M. Burgess. Spacetimes with semantics (i). arXiv:1411.5563, 2014.
[2] M. Burgess. Spacetimes with semantics (ii). arXiv.org:1505.01716, 2015.
[3] M. Burgess. Spacetimes with semantics (iii). arXiv:1608.02193, 2016.
[4] M. Burgess. A spacetime approach to generalized cognitive reasoning in multi-scale learning.
arXiv:1702.04638, 2017.
[5] J.A. Bergstra and M. Burgess. Promise Theory: Principles and Applications (second edition).
χtAxis Press, 2014,2019.
[6] W3C.
Defining n-ary relations on the semantic web:
http://www.w3.org/TR/2004/WD-swbp-n-aryRelations-20040721/.

use

with

individuals.

[7] Mark Burgess. Testing the quantitative spacetime hypothesis using artificial narrative comprehension (i) : Bootstrapping meaning from episodic narrative viewed as a feature landscape, 2020.
[8] Mark Burgess. Testing the quantitative spacetime hypothesis using artificial narrative comprehension (ii) : Establishing the geometry of invariant concepts, themes, and namespaces, 2020.
[9] D. Watt. Programming language syntax and semantics. Prentice Hall, New York, 1991.
[10] B.C. Pierce. Basic Category Theory for Computer Scientists. MIT Press, 1991.
[11] B. Coecke adn A. Kissinger. Picturing Quantum Processes. Cambridge, 2017.
[12] R.W. Langacker. Cognitive Grammar, A Basic Introduction. Oxford, Oxford, 2008.
[13] I. Heim and A. Kratzer. Semantics in Generative Grammar. Blackwell, 1998.
[14] J. Strassner. Handbook of Network and System Administration, chapter Knowledge Engineering
Using Ontologies. Elsevier Handbook, 2007.
[15] G. Deutsche. The Unfolding of Language. Academic Press, 2005.
[16] M.B. Hesse. Forces and Fields: The Concept of Action at a Distance in the History of Physics.
Dover, 1962.
[17] G.R. Grimmett and D.R. Stirzaker. Probability and random processes (3rd edition). Oxford scientific publications, Oxford, 2001.
[18] C.M. Bishop and H. Bishop. Deep Learning. Springer, 2023.
[19] N.M. Cirone, A. Orvieto, B. Walker, C. Salvi, and T. Lyons. Theoretical foundations of deep
selective state-space models, 2025.
[20] J. A. Bergstra. Division by zero, a survey of options. Transmathematica, pages 1–20, 2019.
[21] M. Burgess. Smart Spacetime. χtAxis Press, 2019.
[22] R.P. Feynman. Space-time approach to quantum electrodynamics. Physical Review, 76:769, 1949.
[23] F.J. Dyson. The radiation theories of tomonaga, schwinger and feynman. Physical Review, 75:486,
1949.
[24] L.F. Abbott. Introduction to the background field method. Acta Physica Polonica, B13:33, 1992.
[25] F. Reif. Fundamentals of statistical mechanics. McGraw-Hill, Singapore, 1965.
[26] J. Myrheim. Statistical geometry. CERN preprint TH.2538, August 1978.
[27] R.D. Sorkin. Causal sers: Discrete gravity. arXiv:gr-qc/0309009, 2003.
[28] S. Surya. The causal set approach to quantum gravity. arXiv:1903.11544 [gr-qc], 2019.
[29] E.L. Robertson. Triadic relations: An algebra for the semantic web. Lecture Notes in Computer
Science, 3372:91–108, 2005.
[30] N. Fanizzi, C. d’Amato, and F. Esposito. Evolutionary clustering in description logics: Controlling
concept formation and drift in ontologies. pages 808–821, 09 2008.
[31] Michael Uschold. Ontology and database schema: What’s the difference?
10:243–258, 2015.

Applied Ontology,

[32] R. Milner. The space and motion of communicating agents. Cambridge, 2009.
[33] M. Burgess. In Search of Certainty - The Science of Our Information Infrastructure. χtAxis
Press, November 2013.
[34] A. Couch and M. Burgess. Compass and direction in topic maps. (Oslo University College preprint),
2009.
[35] A. Couch and M. Burgess. Human-understandable inference of causal relationships. In Proc. 1st International Workshop on Knowledge Management for Future Services and Networks, Osaka, Japan,
2010.
[36] Jon M. Kleinberg. Authoritative sources in a hyperlinked environment. Journal of the ACM,
46(5):604–632, 1999.
[37] J. Bjelland, M. Burgess, G. Canright, and K. Engø-Monsen. Eigenvectors of directed graphs and
importance scores: dominance, t-rank, and sink remedies. Data Mining and Knowledge Discovery,
20(1):98–151, 2010.
[38] M. Burgess and A. Couch. On system rollback and totalized fields: An algebraic approach to system
change. J. Log. Algebr. Program., 80(8):427–443, 2011.
[39] C.E. Shannon and W. Weaver. The Mathematical Theory of Communication. University of Illinois
Press, Urbana, 1949.
[40] T.M. Cover and J.A. Thomas. Elements of Information Theory. (J.Wiley & Sons., New York), 1991.
[41] Jan A Bergstra and John V Tucker. Expressing entropy and cross-entropy in expansions of common
meadows, 2025.
[42] Jan A. Bergstra and Alban Ponse. Division by zero in common meadows, 2021.
[43] J.A.D.W. Anderson. Perspex machine ix: Transreal analysis. In Electronic Imaging. San Jos,́ 2007.
[44] J.A.D.W. Anderson, N. Völker, and A. A. Adams. Perspex machine viii, axioms of transreal arithmetic. In Proc. SPIE 6499. Vision Geometry XV, 2007.
[45] J.A. Bergstra and J.V. Tucker. The rational numbers as an abstract datatype. Journal of the ACM,
54:1–25, 2007.
[46] J. A. Bergstra and J. V. Tucker. The data type variety of stack algebras. Annals of Pure and Applied
Logic, 73(1):11–36, 1995.
[47] J. A. Bergstra and J. V. Tucker. Division safe calculation in totalised fields. Theory of Computing
Systems, 43:410–424, 2008.
[48] J. A. Bergstra, Y. Hirshfield, and J. V. Tucker. Meadows and the equational specification of division.
Theoretical Computer Science, 410:1261–1271, 2009.
[49] G.I. Barenblatt. Scaling, self-similarity, and intermediate asymptotics. Cambridge, 1996.
[50] S. Schweber. Relativitsic Quantum Field Theory. Harper & Row, 1961.
[51] H.F. Jones. Groups, Representations, and Physics. Institute of Physics, 1998.
[52] M. Burgess. Notes on trust as a causal basis for social science. SSRN Archive, available at
http://dx.doi.org/10.2139/ssrn.4252501 (DOI: 10.2139/ssrn.4252501), August 2022.
[53] M. Burgess. Trust and trustability: An idealized operational theory of economic attentiveness.
preprint paper (DOI: 10.13140/RG.2.2.26862.28480/1), April 2023.
[54] M. Burgess. Motion of the third kind (i) notes on the causal structure of virtual processes for
privileged observers. DOI: 10.13140/RG.2.2.30483.35361 (notes available on Researchgate), 2021.
[55] M. Burgess. Sstorytime project. https://github.com/markburgess/SSTorytime/
tree/main, 2025.
```
EOF_/home/stevegt/lab/cswg/workshop-2025-07-08-sst-graphical/Semantics_Graphical.txt

File: /home/stevegt/lab/cswg/workshop-2025-07-08-sst-graphical/analysis.md
```
# Analysis of "Agent Semantics, Semantic Spacetime, and Graphical Reasoning"

This paper by Mark Burgess develops a formal model of knowledge
representation based on a graph‐theoretic approach called Semantic
Spacetime. It aims to bridge processes in the physical and conceptual
realms by encoding agents and their interactions as nodes and labeled
links. The key elements of the paper can be summarized as follows:

1. **Semantic Spacetime Model**  
   The model treats spacetime as a collection of agents (nodes)
   connected via directed edges (links) that represent interactions or
   relationships. Each link is associated with one of four irreducible
   types:
   - **Type 0 (NEAR):** Denotes equivalence or proximity without
     direction.
   - **Type ±1 (LEADS TO):** Captures causal or sequential
     relationships.
   - **Type ±2 (CONTAINS):** Reflects spatial or membership
     relations.
   - **Type ±3 (EXPRESSES):** Relates to the expression of properties or
     attributes.

   These link types, based on spacetime characteristics, provide a
   compact language for describing processes and form the basis for the
   paper’s γ(3,4) representation.

2. **Gamma(3,4) Representation and Node Meta-Types**  
   To resolve ambiguities in semantic representation, nodes are
   classified into three meta-types:
   - **Events (e):** Temporary, process-like occurrences.
   - **Things (t):** Persistent entities with material or realized
     aspects.
   - **Concepts (c):** Invariant, abstract ideas or properties.

   The paper outlines interaction rules and allowed transitions (e.g.,
   events can lead to events, things can contain other things, and
   concepts can express properties) using both verbal explanations and
   matrix formulations.

3. **Graphical Algebra and Matrix Representations**  
   The work explores how graphs can be analyzed through their
   adjacency and incidence matrices. These matrix representations help
   formalize concepts such as:
   - **Absorbing States:** Nodes where flows (information or value)
     converge, leading to loss of historical detail.
   - **Eigenvector Analysis:** The application of the Frobenius-Perron
     theorem to determine dominant behavior on the graph, such as
     importance rankings and flow equilibrium.

   These techniques underscore the balance between process dynamics
   (information flow, conservation, and loss) and qualitative semantics.

4. **Application to Arithmetic Processes**  
   An intriguing aspect of the paper is the interpretation of basic
   arithmetic operations (addition, subtraction, multiplication, and
   division) using the graph model. Different interpretations are
   provided:
   - **Addition and Subtraction:** Viewed as both translations along the
     number line (external transformation) and as changes in an agent’s
     internal inventory.
   - **Multiplication and Division:** Here, multiplication is seen as a
     process that duplicates or aggregates, while division is interpreted
     as a sharing or partitioning of an amount among agents.

   In particular, the discussion on division by zero highlights how the
   absorbing nature of certain nodes in a directed graph can be analogous
   to traditional challenges in arithmetic. Various viewpoints are offered,
   such as whether the result should be treated as an infinite flow or a null
   outcome, or whether additional remedial information is needed.

5. **Implications and Broader Connections**  
   The paper concludes by observing that although the Semantic Spacetime
   model simplifies link identification and reduces ambiguity without
   resorting to full ontologies, it also raises deeper questions. Some
   points include:
   - How well the model supports trustworthy and robust knowledge
     representations.
   - The inevitable presence of absorbing states in finite, directed graphs.
   - The potential impact on artificial intelligence in areas where
     context, inference, and dynamic processes are critical.

Overall, Burgess’s work proposes a unified, graph-based framework for
representing both physical and abstract processes. By using a minimal set
of irreducible link types and node meta-types, the model offers a way to
reason about dynamics, conservation, and even arithmetic operations by
emphasizing the interplay between process and meaning.
```
EOF_/home/stevegt/lab/cswg/workshop-2025-07-08-sst-graphical/analysis.md

File: /home/stevegt/lab/cswg/workshop-2025-07-08-sst-graphical/alternative.md
```
# Alternative Perspective: A Simpler Event/State Model

An alternative approach to knowledge representation is to use a 
simpler event/state model where nodes represent states and edges represent 
events. In this view, the inherent complexity of Burgess’s Semantic 
Spacetime model is reduced by abstaining from multiple meta-types for nodes 
and by assigning events solely to links. This section outlines the key ideas 
and contrasts them with the proposed γ(3,4) formulation.

## Core Ideas of the Simplified Model

1. **States as Fundamental Nodes**  
   In the event/state model, every node in the graph is a distinct state.  
   Whether the state is a snapshot of a physical condition, a concept held 
   in a mind, or a representation of an object, it is always treated as a 
   static configuration. There is no supplemental classification into 
   events, things, or concepts.

2. **Events as Transitions (Edges)**  
   All dynamic change is captured exclusively by edges. An edge from state 
   A to state B represents the occurrence of an event that 
   transforms or links A into B. This unifies the semantics of time and 
   causation: every change is an event, regardless of context.

3. **Simplification and Clarity**  
   With states as nodes and events as edges, the representation becomes 
   intuitively similar to classical state-transition diagrams or finite 
   state machines. The model automatically enforces simplicity because 
   it does not require additional labels to capture permanence (as in 
   “things”) or abstract properties (as in “concepts”). Instead, all 
   potentially differing aspects are considered properties of the states 
   or are reflected in the nature of the transitions.

## Comparison with the Semantic Spacetime Approach

- **Burgess’s Model**  
  Burgess’s approach differentiates between transient events, persistent  
  things, and invariant concepts. The four irreducible link types (NEAR,  
  LEADS TO, CONTAINS, EXPRESSES) and the additional γ(3,4) representation add 
  layers of nuance that are intended to capture a wide variety of real-world 
  phenomena. However, this multiplicity can lead to complexity that may not be 
  necessary for all applications.

- **Simpler Event/State Model**  
  In contrast, the event/state model reduces the number of abstractions by:  
  • Modeling every situation as a state.  
  • Representing any change purely as an event along an edge.  
  This reduction in conceptual overhead makes the model more accessible 
  and easier to implement, especially in domains where a coarse view of 
  process dynamics is sufficient.

## Discussion and Implications

The simpler model raises some important reflections:
- **Expressiveness vs. Simplicity:**  
  While Mark Burgess’s Semantic Spacetime model is highly nuanced, it may 
  conflate or over-specify roles that a more elegant event/state model 
  can handle with fewer assumptions.
- **Context and Granularity:**  
  The event/state model is especially attractive when the system under study 
  does not require simultaneous representation of multiple layers of 
  meaning (such as material realization versus conceptual abstraction).
- **Applicability in AI and Process Modeling:**  
  Machine learning models and state-based systems (like finite state 
  machines) often benefit from the inherent simplicity of treating states 
  as nodes and events as the only dynamic change. This simplicity can lead 
  to more robust and interpretable reasoning, particularly during process 
  inference.

In summary, while Burgess’s model offers a sophisticated framework that 
captures subtle distinctions in semantic relationships, a simpler event/state 
model—where nodes are merely states and edges are the events driving 
transitions—may sometimes be more elegant and practical. The choice 
between these approaches ultimately depends on the required level of 
granularity and the specific application in question.
```
EOF_/home/stevegt/lab/cswg/workshop-2025-07-08-sst-graphical/alternative.md

File: /home/stevegt/lab/cswg/workshop-2025-07-08-sst-graphical/README.md
```
class: center, middle

# Workshop Slide Deck: Semantic Spacetime and Graphical Reasoning

Author: Your Name  
CSWG Workshop June 10 2025

---

# Part 1: Analysis

<!-- Begin Analysis Content -->
# Analysis of "Agent Semantics, Semantic Spacetime, and Graphical Reasoning"

This paper by Mark Burgess develops a formal model of knowledge
representation based on a graph‐theoretic approach called Semantic
Spacetime. It aims to bridge processes in the physical and conceptual
realms by encoding agents and their interactions as nodes and labeled
links. The key elements of the paper can be summarized as follows:

1. **Semantic Spacetime Model**  
   The model treats spacetime as a collection of agents (nodes)
   connected via directed edges (links) that represent interactions or
   relationships. Each link is associated with one of four irreducible
   types:
   - **Type 0 (NEAR):** Denotes equivalence or proximity without
     direction.
   - **Type ±1 (LEADS TO):** Captures causal or sequential
     relationships.
   - **Type ±2 (CONTAINS):** Reflects spatial or membership
     relations.
   - **Type ±3 (EXPRESSES):** Relates to the expression of properties or
     attributes.

   These link types, based on spacetime characteristics, provide a
   compact language for describing processes and form the basis for the
   paper’s γ(3,4) representation.

2. **Gamma(3,4) Representation and Node Meta-Types**  
   To resolve ambiguities in semantic representation, nodes are
   classified into three meta-types:
   - **Events (e):** Temporary, process-like occurrences.
   - **Things (t):** Persistent entities with material or realized
     aspects.
   - **Concepts (c):** Invariant, abstract ideas or properties.

   The paper outlines interaction rules and allowed transitions (e.g.,
   events can lead to events, things can contain other things, and
   concepts can express properties) using both verbal explanations and
   matrix formulations.

3. **Graphical Algebra and Matrix Representations**  
   The work explores how graphs can be analyzed through their
   adjacency and incidence matrices. These matrix representations help
   formalize concepts such as:
   - **Absorbing States:** Nodes where flows (information or value)
     converge, leading to loss of historical detail.
   - **Eigenvector Analysis:** The application of the Frobenius-Perron
     theorem to determine dominant behavior on the graph, such as
     importance rankings and flow equilibrium.

   These techniques underscore the balance between process dynamics
   (information flow, conservation, and loss) and qualitative semantics.

4. **Application to Arithmetic Processes**  
   An intriguing aspect of the paper is the interpretation of basic
   arithmetic operations (addition, subtraction, multiplication, and
   division) using the graph model. Different interpretations are
   provided:
   - **Addition and Subtraction:** Viewed as both translations along the
     number line (external transformation) and as changes in an agent’s
     internal inventory.
   - **Multiplication and Division:** Here, multiplication is seen as a
     process that duplicates or aggregates, while division is interpreted
     as a sharing or partitioning of an amount among agents.

   In particular, the discussion on division by zero highlights how the
   absorbing nature of certain nodes in a directed graph can be analogous
   to traditional challenges in arithmetic. Various viewpoints are offered,
   such as whether the result should be treated as an infinite flow or a null
   outcome, or whether additional remedial information is needed.

5. **Implications and Broader Connections**  
   The paper concludes by observing that although the Semantic Spacetime
   model simplifies link identification and reduces ambiguity without
   resorting to full ontologies, it also raises deeper questions. Some
   points include:
   - How well the model supports trustworthy and robust knowledge
     representations.
   - The inevitable presence of absorbing states in finite, directed graphs.
   - The potential impact on artificial intelligence in areas where
     context, inference, and dynamic processes are critical.

Overall, Burgess’s work proposes a unified, graph-based framework for
representing both physical and abstract processes. By using a minimal set
of irreducible link types and node meta-types, the model offers a way to
reason about dynamics, conservation, and even arithmetic operations by
emphasizing the interplay between process and meaning.
<!-- End Analysis Content -->

---

# Part 2: Alternative Perspective

<!-- Begin Alternative Content -->
# Alternative Perspective: A Simpler Event/State Model

An alternative approach to knowledge representation is to use a 
simpler event/state model where nodes represent states and edges represent 
events. In this view, the inherent complexity of Burgess’s Semantic 
Spacetime model is reduced by abstaining from multiple meta-types for nodes 
and by assigning events solely to links. This section outlines the key ideas 
and contrasts them with the proposed γ(3,4) formulation.

## Core Ideas of the Simplified Model

1. **States as Fundamental Nodes**  
   In the event/state model, every node in the graph is a distinct state.  
   Whether the state is a snapshot of a physical condition, a concept held 
   in a mind, or a representation of an object, it is always treated as a 
   static configuration. There is no supplemental classification into 
   events, things, or concepts.

2. **Events as Transitions (Edges)**  
   All dynamic change is captured exclusively by edges. An edge from state 
   A to state B represents the occurrence of an event that trans-
   forms or links A into B. This unifies the semantics of time and causation:
   every change is an event, regardless of context.

3. **Simplification and Clarity**  
   With states as nodes and events as edges, the representation becomes 
   intuitively similar to classical state-transition diagrams or finite
   state machines. The model automatically enforces simplicity because it
   does not require additional labels to capture permanence (as in “things”)
   or abstract properties (as in “concepts”). Instead, all potentially
   differing aspects are considered properties of the states or are reflected
   in the nature of the transitions.

## Comparison with the Semantic Spacetime Approach

- **Burgess’s Model**  
  Burgess’s approach differentiates between transient events, persistent 
  things, and invariant concepts. The four irreducible link types (NEAR,
  LEADS TO, CONTAINS, EXPRESSES) and the additional γ(3,4) representation add 
  layers of nuance that are intended to capture a wide variety of real-world
  phenomena. However, this multiplicity can lead to complexity that may not be
  necessary for all applications.

- **Simpler Event/State Model**  
  In contrast, the event/state model reduces the number of abstractions by:  
  • Modeling every situation as a state.  
  • Representing any change purely as an event along an edge.  
  This reduction in conceptual overhead makes the model more accessible
  and easier to implement, especially in domains where a coarse view of
  process dynamics is sufficient.

## Discussion and Implications

The simpler model raises some important reflections:
- **Expressiveness vs. Simplicity:**  
  While Mark Burgess’s Semantic Spacetime model is highly nuanced, it may
  conflate or over-specify roles that a more elegant event/state model can handle
  with fewer assumptions.
- **Context and Granularity:**  
  The event/state model is especially attractive when the system under study
  does not require simultaneous representation of multiple layers of meaning
  (such as material realization versus conceptual abstraction).
- **Applicability in AI and Process Modeling:**  
  Machine learning models and state-based systems (like finite state machines)
  often benefit from the inherent simplicity of treating states as nodes and
  events as the only dynamic change. This simplicity can lead to more robust and
  interpretable reasoning, particularly during process inference.

In summary, while Burgess’s model offers a sophisticated framework that
captures subtle distinctions in semantic relationships, a simpler event/state
model—where nodes are merely states and edges are the events driving
transitions—may sometimes be more elegant and practical. The choice
between these approaches ultimately depends on the required level of
granularity and the specific application in question.
<!-- End Alternative Content -->

---

class: center, middle

# Thank You!
```
EOF_/home/stevegt/lab/cswg/workshop-2025-07-08-sst-graphical/README.md